{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attempt_one.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamkaustubh1019/Deep_Learning/blob/master/Attempt_one.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op8CHr5TrjIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "from os.path import join\n",
        "\n",
        "# dataset params\n",
        "def get_train_config(json_path):\n",
        "    with open(json_path) as json_file:\n",
        "        config = json.load(json_file)\n",
        "    return config\n",
        "\n",
        "def get_train_configs(json_path):\n",
        "    with open(json_path) as json_file:\n",
        "        config = json.load(json_file)\n",
        "        # find free hyper-params\n",
        "        free_hp = []\n",
        "        free_fields = []\n",
        "        for field in config:\n",
        "            if isinstance(config[field], dict): # free hyper-param\n",
        "                free_fields.append(field)\n",
        "                free_vs = []\n",
        "                for v in config[field].values():\n",
        "                    free_vs.append(v)\n",
        "                free_hp.append(free_vs)\n",
        "\n",
        "        if len(free_fields) == 0:\n",
        "            return [config], ['.']\n",
        "\n",
        "        # generate all combinations of hp\n",
        "        free_hp_comb_list = list(itertools.product(*free_hp))\n",
        "\n",
        "        # write the combination back to dict object\n",
        "        ret_config_list = []\n",
        "        ret_rel_path_list = []\n",
        "        for free_hp_comb in free_hp_comb_list:\n",
        "            ret_config = dict(config)\n",
        "            rel_path = []\n",
        "            # modify the free field with the value in each combination\n",
        "            for field, v in zip(free_fields, free_hp_comb):\n",
        "                ret_config[field] = v\n",
        "                rel_path.append(field+'_'+str(v).replace('[', '').replace(']',''))\n",
        "            ret_config_list.append(ret_config)\n",
        "            ret_rel_path_list.append(join(*rel_path))\n",
        "    return ret_config_list, ret_rel_path_list\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p78m8Z8er2bY",
        "colab_type": "text"
      },
      "source": [
        "Eval Files Execution shall start from here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1KPR_s6r_8v",
        "colab_type": "text"
      },
      "source": [
        "eval/cupy.utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMavccTer6b_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright (C) 2018  Mikel Artetxe <artetxem@gmail.com>\n",
        "#\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "import numpy\n",
        "\n",
        "try:\n",
        "    import cupy\n",
        "except ImportError:\n",
        "    cupy = None\n",
        "\n",
        "\n",
        "def supports_cupy():\n",
        "    return cupy is not None\n",
        "\n",
        "\n",
        "def get_cupy():\n",
        "    return cupy\n",
        "\n",
        "\n",
        "def get_array_module(x):\n",
        "    if cupy is not None:\n",
        "        return cupy.get_array_module(x)\n",
        "    else:\n",
        "        return numpy\n",
        "\n",
        "\n",
        "def asnumpy(x):\n",
        "    if cupy is not None:\n",
        "        return cupy.asnumpy(x)\n",
        "    else:\n",
        "        return numpy.asarray(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIxVPDlXsEtG",
        "colab_type": "text"
      },
      "source": [
        "eval/embeddings.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G2eOJipsHr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright (C) 2016-2018  Mikel Artetxe <artetxem@gmail.com>\n",
        "#\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def read(file, threshold=0, vocabulary=None, dtype='float'):\n",
        "    header = file.readline().strip().split(' ')\n",
        "    count = int(header[0]) if threshold <= 0 else min(threshold, int(header[0]))\n",
        "    dim = int(header[1])\n",
        "    words = []\n",
        "    matrix = np.empty((count, dim), dtype=dtype) if vocabulary is None else []\n",
        "    for i in range(count):\n",
        "        word, vec = file.readline().split(' ', 1)\n",
        "        if vocabulary is None:\n",
        "            words.append(word)\n",
        "            matrix[i] = np.fromstring(vec, sep=' ', dtype=dtype)\n",
        "        elif word in vocabulary:\n",
        "            words.append(word)\n",
        "            matrix.append(np.fromstring(vec, sep=' ', dtype=dtype))\n",
        "\n",
        "    return (words, matrix) if vocabulary is None else (words, np.array(matrix, dtype=dtype))\n",
        "\n",
        "\n",
        "def write(words, matrix, file):\n",
        "    m = asnumpy(matrix)\n",
        "    print('%d %d' % m.shape, file=file)\n",
        "    for i in range(len(words)):\n",
        "        print(words[i] + ' ' + ' '.join(['%.6g' % x for x in m[i]]), file=file)\n",
        "\n",
        "\n",
        "def length_normalize(matrix):\n",
        "    xp = get_array_module(matrix)\n",
        "    norms = xp.sqrt(xp.sum(matrix**2, axis=1))\n",
        "    norms[norms == 0] = 1\n",
        "    return matrix / norms[:, xp.newaxis]\n",
        "\n",
        "\n",
        "def mean_center(matrix):\n",
        "    xp = get_array_module(matrix)\n",
        "    avg = xp.mean(matrix, axis=0)\n",
        "    return matrix - avg\n",
        "\n",
        "\n",
        "def length_normalize_dimensionwise(matrix):\n",
        "    xp = get_array_module(matrix)\n",
        "    norms = xp.sqrt(xp.sum(matrix**2, axis=0))\n",
        "    norms[norms == 0] = 1\n",
        "    return matrix / norms\n",
        "\n",
        "\n",
        "def mean_center_embeddingwise(matrix):\n",
        "    xp = get_array_module(matrix)\n",
        "    avg = xp.mean(matrix, axis=1)\n",
        "    return matrix - avg[:, xp.newaxis]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhsGK4QbsO2Y",
        "colab_type": "text"
      },
      "source": [
        "eval_translation.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmQJbkQtsR31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "fd9ec18b-adca-4cab-e33a-158e874a66fc"
      },
      "source": [
        "# Copyright (C) 2016-2018  Mikel Artetxe <artetxem@gmail.com>\n",
        "#\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "!pip install embeddings\n",
        "import embeddings\n",
        "import argparse\n",
        "import collections\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "\n",
        "BATCH_SIZE = 500\n",
        "\n",
        "\n",
        "def main(src_embeddings,trg_embeddings):\n",
        "    # Parse command line arguments\n",
        "    parser = argparse.ArgumentParser(description='Evaluate embeddings of two languages in a shared space in word translation induction')\n",
        "    parser.add_argument('src_embeddings', help='the source language embeddings')\n",
        "    parser.add_argument('trg_embeddings', help='the target language embeddings')\n",
        "    parser.add_argument('-d', '--dictionary', default=sys.stdin.fileno(), help='the test dictionary file (defaults to stdin)')\n",
        "    parser.add_argument('--retrieval', default='nn', choices=['nn', 'invnn', 'invsoftmax'], help='the retrieval method (nn: standard nearest neighbor; invnn: inverted nearest neighbor; invsoftmax: inverted softmax)')\n",
        "    parser.add_argument('--inv_temperature', default=1, type=float, help='the inverse temperature (only compatible with inverted softmax)')\n",
        "    parser.add_argument('--inv_sample', default=None, type=int, help='use a random subset of the source vocabulary for the inverse computations (only compatible with inverted softmax)')\n",
        "    parser.add_argument('--dot', action='store_true', help='use the dot product in the similarity computations instead of the cosine')\n",
        "    parser.add_argument('--encoding', default='utf-8', help='the character encoding for input/output (defaults to utf-8)')\n",
        "    parser.add_argument('--seed', type=int, default=0, help='the random seed')\n",
        "    parser.add_argument('--precision', choices=['fp16', 'fp32', 'fp64'], default='fp64', help='the floating-point precision (defaults to fp64)')\n",
        "    parser.add_argument('--cuda', action='store_true', help='use cuda (requires cupy)')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Choose the right dtype for the desired precision\n",
        "    if args.precision == 'fp16':\n",
        "        dtype = 'float16'\n",
        "    elif args.precision == 'fp32':\n",
        "        dtype = 'float32'\n",
        "    elif args.precision == 'fp64':\n",
        "        dtype = 'float64'\n",
        "\n",
        "    # Read input embeddings\n",
        "    srcfile = open(args.src_embeddings, encoding=args.encoding, errors='surrogateescape')\n",
        "    trgfile = open(args.trg_embeddings, encoding=args.encoding, errors='surrogateescape')\n",
        "    src_words, x = embeddings.read(srcfile, dtype=dtype)\n",
        "    trg_words, z = embeddings.read(trgfile, dtype=dtype)\n",
        "\n",
        "    # NumPy/CuPy management\n",
        "    if args.cuda:\n",
        "        if not supports_cupy():\n",
        "            print('ERROR: Install CuPy for CUDA support', file=sys.stderr)\n",
        "            sys.exit(-1)\n",
        "        xp = get_cupy()\n",
        "        x = xp.asarray(x)\n",
        "        z = xp.asarray(z)\n",
        "    else:\n",
        "        xp = np\n",
        "    xp.random.seed(args.seed)\n",
        "\n",
        "    # Length normalize embeddings so their dot product effectively computes the cosine similarity\n",
        "    if not args.dot:\n",
        "        x = embeddings.length_normalize(x)\n",
        "        z = embeddings.length_normalize(z)\n",
        "\n",
        "    # Build word to index map\n",
        "    src_word2ind = {word: i for i, word in enumerate(src_words)}\n",
        "    trg_word2ind = {word: i for i, word in enumerate(trg_words)}\n",
        "\n",
        "    # Read dictionary and compute coverage\n",
        "    f = open(args.dictionary, encoding=args.encoding, errors='surrogateescape')\n",
        "    src2trg = collections.defaultdict(set)\n",
        "    oov = set()\n",
        "    vocab = set()\n",
        "    for line in f:\n",
        "        src, trg = line.split()\n",
        "        try:\n",
        "            src_ind = src_word2ind[src]\n",
        "            trg_ind = trg_word2ind[trg]\n",
        "            src2trg[src_ind].add(trg_ind)\n",
        "            vocab.add(src)\n",
        "        except KeyError:\n",
        "            oov.add(src)\n",
        "    src = list(src2trg.keys())\n",
        "    oov -= vocab  # If one of the translation options is in the vocabulary, then the entry is not an oov\n",
        "    coverage = len(src2trg) / (len(src2trg) + len(oov))\n",
        "\n",
        "    # Find translations\n",
        "    translation = collections.defaultdict(int)\n",
        "    if args.retrieval == 'nn':  # Standard nearest neighbor\n",
        "        for i in range(0, len(src), BATCH_SIZE):\n",
        "            j = min(i + BATCH_SIZE, len(src))\n",
        "            similarities = x[src[i:j]].dot(z.T)\n",
        "            nn = similarities.argmax(axis=1).tolist()\n",
        "            for k in range(j-i):\n",
        "                translation[src[i+k]] = nn[k]\n",
        "    elif args.retrieval == 'invnn':  # Inverted nearest neighbor\n",
        "        best_rank = np.full(len(src), x.shape[0], dtype=int)\n",
        "        best_sim = np.full(len(src), -100, dtype=dtype)\n",
        "        for i in range(0, z.shape[0], BATCH_SIZE):\n",
        "            j = min(i + BATCH_SIZE, z.shape[0])\n",
        "            similarities = z[i:j].dot(x.T)\n",
        "            ind = (-similarities).argsort(axis=1)\n",
        "            ranks = asnumpy(ind.argsort(axis=1)[:, src])\n",
        "            sims = asnumpy(similarities[:, src])\n",
        "            for k in range(i, j):\n",
        "                for l in range(len(src)):\n",
        "                    rank = ranks[k-i, l]\n",
        "                    sim = sims[k-i, l]\n",
        "                    if rank < best_rank[l] or (rank == best_rank[l] and sim > best_sim[l]):\n",
        "                        best_rank[l] = rank\n",
        "                        best_sim[l] = sim\n",
        "                        translation[src[l]] = k\n",
        "    elif args.retrieval == 'invsoftmax':  # Inverted softmax\n",
        "        sample = xp.arange(x.shape[0]) if args.inv_sample is None else xp.random.randint(0, x.shape[0], args.inv_sample)\n",
        "        partition = xp.zeros(z.shape[0])\n",
        "        for i in range(0, len(sample), BATCH_SIZE):\n",
        "            j = min(i + BATCH_SIZE, len(sample))\n",
        "            partition += xp.exp(args.inv_temperature*z.dot(x[sample[i:j]].T)).sum(axis=1)\n",
        "        for i in range(0, len(src), BATCH_SIZE):\n",
        "            j = min(i + BATCH_SIZE, len(src))\n",
        "            p = xp.exp(args.inv_temperature*x[src[i:j]].dot(z.T)) / partition\n",
        "            nn = p.argmax(axis=1).tolist()\n",
        "            for k in range(j-i):\n",
        "                translation[src[i+k]] = nn[k]\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = np.mean([1 if translation[i] in src2trg[i] else 0 for i in src])\n",
        "    print('Coverage:{0:7.2%}  Accuracy:{1:7.2%}'.format(coverage, accuracy))\n",
        "    sys.exit(\"{0:.2f}\".format(float(100)*accuracy))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting embeddings\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/89/850eede11ae8e0b7b644473910e7c780804092e01cb343c00eb1d60f9d4e/embeddings-0.0.6.tar.gz\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from embeddings) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from embeddings) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from embeddings) (1.16.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->embeddings) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->embeddings) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->embeddings) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->embeddings) (2019.3.9)\n",
            "Building wheels for collected packages: embeddings\n",
            "  Building wheel for embeddings (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/7a/c7/29dae66bb181e6a9df35639e74c075c17b6627792077ab4696\n",
            "Successfully built embeddings\n",
            "Installing collected packages: embeddings\n",
            "Successfully installed embeddings-0.0.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [-d DICTIONARY]\n",
            "                             [--retrieval {nn,invnn,invsoftmax}]\n",
            "                             [--inv_temperature INV_TEMPERATURE]\n",
            "                             [--inv_sample INV_SAMPLE] [--dot]\n",
            "                             [--encoding ENCODING] [--seed SEED]\n",
            "                             [--precision {fp16,fp32,fp64}] [--cuda]\n",
            "                             src_embeddings trg_embeddings\n",
            "ipykernel_launcher.py: error: the following arguments are required: trg_embeddings\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9gf6gSA0as6",
        "colab_type": "text"
      },
      "source": [
        "UTILS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmBg4PMZ0cfs",
        "colab_type": "text"
      },
      "source": [
        "Call Backs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nx1Gjkg0eJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "class EarlyStopper():\n",
        "    def __init__(self, patience, min_delta=1e-5):\n",
        "        self.hist_loss = defaultdict(float)\n",
        "        self.patience_cnt = 0\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "    def check_early_stop(self, cur_loss, epoch):\n",
        "        '''check if we should early stop'''\n",
        "        self.hist_loss[epoch] = cur_loss\n",
        "        # if epoch > 0 and self.hist_loss[epoch-1] - cur_loss > self.min_delta:\n",
        "        if epoch > 0 and cur_loss < self.best_loss - self.min_delta:\n",
        "            self.patience_cnt = 0\n",
        "            self.best_loss = cur_loss\n",
        "        else:\n",
        "            self.patience_cnt += 1\n",
        "        if self.patience_cnt > self.patience:\n",
        "            print(\"early stopping...\")\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "class ModelChecker():\n",
        "    def __init__(self, min_delta=1e-5):\n",
        "        self.best_loss = float('inf')\n",
        "        self.epoch_loss = defaultdict(list)\n",
        "        self.min_delta = min_delta\n",
        "\n",
        "    def record_loss(self, cur_loss, epoch):\n",
        "        self.epoch_loss[epoch].append(cur_loss)\n",
        "\n",
        "    def get_best_loss(self):\n",
        "        return self.best_loss\n",
        "\n",
        "    def check_for_best(self, cur_loss, epoch):\n",
        "        cur_loss = sum(self.epoch_loss[epoch]) / float(len(self.epoch_loss[epoch]))\n",
        "        if epoch > 0 and cur_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = cur_loss\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "def SGDWR(T_total, T_0, T_mult, lr_max, lr_min): \n",
        "    # Use \"STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS\"\n",
        "    # return a dictionary records at each epoch the learning rate: {<#epoch>:<lr>}\n",
        "    lr_dict = {}\n",
        "    T_cur = 0.0 # epochs since last restart\n",
        "    T_i = float(T_0)\n",
        "    for T in range(T_total+1):\n",
        "        if T_cur > T_i:\n",
        "            T_cur = 0.0\n",
        "            T_i = T_i * T_mult\n",
        "        lr = lr_min + 0.5*(lr_max-lr_min)*(1+math.cos(math.pi*(T_cur/T_i)))\n",
        "        T_cur += 1\n",
        "        lr_dict[T] = lr\n",
        "    return lr_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RFOaK1I11Xi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "2f01f02a-f368-4232-a484-15531bff4704"
      },
      "source": [
        "!unsup-cross-lingual-embedding-transfer-master/data/download.sh"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   278    0   278    0     0    776      0 --:--:-- --:--:-- --:--:--   776\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   278    0   278    0     0    920      0 --:--:-- --:--:-- --:--:--   917\n",
            "bg\n",
            "/dictionaries/en-bg.txt\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   297    0   297    0     0   4640      0 --:--:-- --:--:-- --:--:--  4640\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   297    0   297    0     0   4500      0 --:--:-- --:--:-- --:--:--  4500\n",
            "/dictionaries/en-bg.0-5000.txt\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   297    0   297    0     0   4367      0 --:--:-- --:--:-- --:--:--  4367\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   297    0   297    0     0   3228      0 --:--:-- --:--:-- --:--:--  3228\n",
            "/dictionaries/en-bg.5000-6500.txt\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   297    0   297    0     0   4640      0 --:--:-- --:--:-- --:--:--  4640\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   297    0   297    0     0   4640      0 --:--:-- --:--:-- --:--:--  4640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcgeXkUL0j1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXpCB_A0iE6",
        "colab_type": "text"
      },
      "source": [
        "Data Helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebXxqsfK0mox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "95bb86db-105a-4381-dbcf-32bfe3a64754"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "\n",
        "\"\"\"\n",
        "USAGE:\n",
        "OUTPUT:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from os import listdir \n",
        "from os.path import join, isdir\n",
        "from tqdm import tqdm\n",
        "tqdm.monitor_interval = 0\n",
        "from numpy import linalg as LA\n",
        "from collections import defaultdict\n",
        "import errno\n",
        "from sklearn import preprocessing\n",
        "from scipy import stats\n",
        "from itertools import cycle\n",
        "\n",
        "def _handle_zeros_in_scale(scale, copy=True):\n",
        "    ''' Makes sure that whenever scale is zero, we handle it correctly.\n",
        "    This happens in most scalers when we have constant features.'''\n",
        "\n",
        "    # if we are fitting on 1D arrays, scale might be a scalar\n",
        "    if np.isscalar(scale):\n",
        "        if scale == .0:\n",
        "            scale = 1.\n",
        "        return scale\n",
        "    elif isinstance(scale, np.ndarray):\n",
        "        if copy:\n",
        "            # New array to avoid side-effects\n",
        "            scale = scale.copy()\n",
        "        scale[scale == 0.0] = 1.0\n",
        "        return scale\n",
        "\n",
        "def my_scale(X, axis=0):\n",
        "    '''From scikit-learn preprocessing.scale'''\n",
        "    Xr = np.asarray(X)\n",
        "    mean_ = np.mean(X, axis)\n",
        "    scale_ = np.std(X, axis)\n",
        "    scale_ = _handle_zeros_in_scale(scale_, copy=False)\n",
        "\n",
        "    Xr -= mean_\n",
        "    Xr /= scale_\n",
        "\n",
        "    return Xr, mean_, scale_\n",
        "\n",
        "def mkdir_p(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def file_len(fname):\n",
        "    with open(fname, errors='surrogateescape') as f:\n",
        "        for i, l in enumerate(f):\n",
        "            pass\n",
        "    return i + 1\n",
        "\n",
        "def load_bi_dict(fname, splitter='\\t'):\n",
        "    bi_dict = defaultdict(list)\n",
        "    for l in open(fname, errors='surrogateescape'):\n",
        "        if splitter in l:\n",
        "            ss = l.strip().split(splitter)\n",
        "        else:\n",
        "            ss = l.strip().split(' ')\n",
        "        bi_dict[ss[0]].append(ss[1])\n",
        "    return bi_dict\n",
        "\n",
        "def load_text_vec(fname, splitter=' ', vocab_size=None, top_n=None, norm=None):\n",
        "    \"\"\"\n",
        "    Load dx1 word vecs from word2vec-like format:\n",
        "    <word1> <dim1> <dim2> ...\n",
        "    <word2> <dim1> <dim2> ...\n",
        "    ...\n",
        "    \"\"\"\n",
        "    # word_vecs = defaultdict(list)\n",
        "    word_vecs = dict()\n",
        "    words = []\n",
        "    vecs = []\n",
        "    with open(fname, \"r\", errors='surrogateescape') as f:\n",
        "        if vocab_size is None:     \n",
        "            vocab_size = file_len(fname)\n",
        "        layer1_size = None\n",
        "        vocab_count = 0\n",
        "\n",
        "        for line in tqdm(f.readlines()[:min(vocab_size,top_n)+1]):\n",
        "            ss = line.split(' ')\n",
        "            if len(ss) <= 3:\n",
        "                continue\n",
        "            word = ss[0]\n",
        "            dims = ' '.join(ss[1:]).strip().split(splitter)\n",
        "            if layer1_size is None:\n",
        "                layer1_size = len(dims)\n",
        "                # print dims\n",
        "                print(\"reading word2vec at vocab_size:%d, dimension:%d\" % (vocab_size, layer1_size))\n",
        "\n",
        "            vec = np.fromstring(' '.join(dims), dtype='float32', count=layer1_size, sep=' ')\n",
        "            vecs.append(vec)\n",
        "            words.append(word)\n",
        "            vocab_count += 1\n",
        "            if vocab_count >= vocab_size:\n",
        "                break\n",
        "            if top_n is not None:\n",
        "                if vocab_count >= top_n:\n",
        "                    break\n",
        "\n",
        "        vecs = np.asarray(np.stack(vecs, axis=0))\n",
        "        if norm == 'scale':\n",
        "            vecs = preprocessing.scale(vecs)\n",
        "        elif norm == 'l2':\n",
        "            vecs = preprocessing.normalize(vecs, norm='l2', axis=1)\n",
        "        elif norm == 'l2+mean_center':\n",
        "            vecs = preprocessing.normalize(vecs, norm='l2', axis=1)\n",
        "            vecs = mean_center(vecs)\n",
        "        elif norm == 'none':\n",
        "            pass\n",
        "        else:\n",
        "            print('ERROR: unrecoginized norm optioin {}'.format(norm))\n",
        "        # print(\"vecs.mean(axis=0)\", vecs.mean(axis=0))\n",
        "\n",
        "        scaled_vecs, mean, std = my_scale(vecs)\n",
        "\n",
        "        # vecs = np.copy(scaled_vecs)\n",
        "        # vecs = preprocessing.normalize(vecs, norm='l2', axis=1)\n",
        "        # vecs *= std\n",
        "        # vecs += mean\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            word_vecs[word] = vecs[i,:]\n",
        "\n",
        "        print('vecs.shape:', vecs.shape)\n",
        "    return vocab_size, word_vecs, words, vecs, scaled_vecs, mean, std, layer1_size\n",
        "\n",
        "# def cycle_zip(A, B): # return zip of list A,B, which are in different sizes \n",
        "#     return zip(A, cycle(B)) if len(A) > len(B) else zip(cycle(A), B)\n",
        "\n",
        "def downsample_frequent_words(counts, total_count, frequency_threshold=1e-3):\n",
        "    if total_count > 1: # if inputs are counts\n",
        "        threshold_count = float(frequency_threshold * total_count)\n",
        "        probs = (np.sqrt(counts / threshold_count) + 1) * (threshold_count / counts)\n",
        "        probs = np.maximum(probs, 1.0)    #Zm: Originally maximum, which upsamples rare words\n",
        "        probs *= counts\n",
        "        probs /= probs.sum()\n",
        "    elif total_count <= 1: # inputs are frequency already\n",
        "        probs = np.power(counts, 0.75)\n",
        "        probs /= probs.sum()\n",
        "    return probs\n",
        "\n",
        "def load_freq(p, top_n=None, splitter=' '):\n",
        "    \"\"\"\n",
        "    Load word frequence from word count\n",
        "    <word1> <count1> ...\n",
        "    <word2> <count2> ...\n",
        "    ...\n",
        "    return <word:word_count dictionary>, <freq_list>\n",
        "    \"\"\"\n",
        "    if p is None:\n",
        "        # assume uniform distribution\n",
        "        return None, np.ones(top_n)/top_n \n",
        "\n",
        "        # assume Zipf's law\n",
        "        # freq = 1/(np.arange(top_n)+1)\n",
        "        # return None, downsample_frequent_words(freq/np.sum(freq), 0)\n",
        "\n",
        "    else:\n",
        "        w_count = defaultdict(list)\n",
        "        count_list = []\n",
        "        for l in open(p):\n",
        "            ss = l.strip().split(splitter)\n",
        "            w_count[ss[0]].append(float(ss[1]))\n",
        "            count_list.append(float(ss[1]))\n",
        "            if len(count_list) >= top_n:\n",
        "                break\n",
        "        counts = np.asarray(count_list, dtype=np.float32)\n",
        "        total_count = counts.sum()\n",
        "    return w_count, downsample_frequent_words(counts, total_count)\n",
        "\n",
        "def uniform_batch_iter(xs, batch_size=32, num_epochs=1, shuffle=True, full_batch=True, verbose=False):\n",
        "    \"\"\"\n",
        "    Generates a batch iterator for a dataset. Use uniform sample from data.\n",
        "    xs: list of numpy array data, must has equal size on axis 0\n",
        "    batch_size: size of batch\n",
        "    num_epochs: number of epochs\n",
        "    shuffle: True will shuffle xs with same random indices, False will not shuffle\n",
        "    full_batch: True to make use every batch has the same batch_size, False the last batch of each epoch may contain samples less than batch_size\n",
        "    verbose: True to print epoch and batch size info on the run \n",
        "    \"\"\"\n",
        "\n",
        "    data_sizes = [x.shape[0] for x in xs]\n",
        "    assert data_sizes[1:] == data_sizes[:-1] # make sure they are all equal\n",
        "    data_size = data_sizes[0]\n",
        "    num_batches_per_epoch = int((data_size-0.1)/batch_size) + 1\n",
        "    # make data be n times size of batch size\n",
        "    res_size = batch_size - data_size % batch_size\n",
        "    if res_size > 0 and full_batch:\n",
        "        shuffle_indices = np.random.choice(np.arange(data_size), size=res_size, replace=False)\n",
        "        xs = [np.concatenate((x, x[shuffle_indices]), axis=0) for x in xs]\n",
        "        data_size = xs[0].shape[0]\n",
        "    # print(num_epochs*num_batches_per_epoch)\n",
        "    for epoch in range(num_epochs):\n",
        "        if shuffle: # Shuffle the data at each epoch\n",
        "            xs_shuffled = list()\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size)) # same shuffle for all data\n",
        "            for i,x in enumerate(xs):    \n",
        "                xs_shuffled.append(x[shuffle_indices])\n",
        "        else:\n",
        "            xs_shuffled = xs\n",
        "        if verbose:\n",
        "            sys.stdout.write(\"\\rIn epoch >> \" + str(epoch + 1))\n",
        "            sys.stdout.flush()\n",
        "            # print(\"In epoch >> \" + str(epoch + 1), end='', flush=True)\n",
        "            # print(\"num batches per epoch is: \" + str(num_batches_per_epoch), end='', flush=True)\n",
        "        \n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            xs_batch = [x[start_index:end_index] for x in xs_shuffled]\n",
        "            batch = (xs_batch, start_index, end_index, epoch)\n",
        "            yield batch\n",
        "\n",
        "def freq_sample_batch_iter(xs, ps, batch_size=32, num_epochs=1, verbose=False):\n",
        "    \"\"\"\n",
        "    Generates a batch iterator for a dataset. Use frequence sample from ps.\n",
        "    \"\"\"\n",
        "    data_size = max([x.shape[0] for x in xs]) # find the max data size\n",
        "    num_batches_per_epoch = int((data_size-0.1)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        if verbose:\n",
        "            print(\"In epoch >> \" + str(epoch + 1))\n",
        "            print(\"num batches per epoch is: \" + str(num_batches_per_epoch))\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            xs_batch = []\n",
        "            for x, p in zip(xs, ps):\n",
        "                idx = np.random.choice(np.arange(x.shape[0]), size=batch_size, replace=True, p=p)\n",
        "                xs_batch.append(x[idx,:])\n",
        "            yield (xs_batch, None, None)\n",
        "\n",
        "\n",
        "# utility function\n",
        "def mean_center(matrix, axis=0):\n",
        "    # print(type(matrix))\n",
        "    if type(matrix) is np.ndarray:\n",
        "        avg = np.mean(matrix, axis=axis, keepdims=True)\n",
        "    else:\n",
        "        avg = torch.mean(matrix, axis=axis, keepdims=True)\n",
        "    return matrix - avg\n",
        "\n",
        "def get_wordvec(filename, top_n=None):\n",
        "    arr = []\n",
        "    words = []\n",
        "    count = 0\n",
        "    for num, line in enumerate(open(filename)):\n",
        "        if num == 0 and len(line.split()) < 4:\n",
        "            continue\n",
        "        word, vect = line.rstrip().split(' ', 1)\n",
        "        arr.append(vect.split())\n",
        "        # assert len(vect.split()) == 300\n",
        "        words.append(word)\n",
        "        count += 1\n",
        "        if top_n is not None and count >= top_n:\n",
        "            break\n",
        "    return np.vstack(arr).astype(float), words\n",
        "    # return mean_center(np.vstack(arr).astype(float), axis=0), words\n",
        "\n",
        "def save_emb(filename, X, words):\n",
        "    \"\"\"\n",
        "    save vectors X(np.array) and words to file f\n",
        "    \"\"\"\n",
        "    f = open(filename, 'w', errors='surrogateescape')\n",
        "    num_words = X.shape[0]\n",
        "    assert num_words == len(words), \"saved words is not aligned with saved vectors\"\n",
        "    f.write('{} {}\\n'.format(num_words, X.shape[1]))\n",
        "    for i,word in enumerate(words):\n",
        "        word_vec_str = ' '.join(map(str, X[i,:].tolist()))\n",
        "        f.write(\"{} {}\\n\".format(word, word_vec_str))\n",
        "\n",
        "def get_dictionary_index(filename, src_words, tgt_words, limit=None, unique=False):\n",
        "    src_idx, tgt_idx = [], []\n",
        "    counter = 0\n",
        "    for i,l in enumerate(open(filename).readlines()):\n",
        "        if limit is not None and counter >= limit:\n",
        "            break\n",
        "        ss = l.strip().split()\n",
        "        if ss[0].lower() in src_words and ss[1].lower() in tgt_words:\n",
        "            if unique and (src_words.index(ss[0].lower()) in src_idx or tgt_words.index(ss[1].lower()) in tgt_idx):\n",
        "                continue\n",
        "            src_idx.append(src_words.index(ss[0].lower()))\n",
        "            tgt_idx.append(tgt_words.index(ss[1].lower()))\n",
        "            counter += 1\n",
        "    return src_idx, tgt_idx\n",
        "\n",
        "def get_dictionary_matrix(filename, src_words, tgt_words, limit=None):\n",
        "    \"\"\"\n",
        "    Suppose the dictionary has line format: <src_word> <tgt_word>\n",
        "    \"\"\"\n",
        "    F = np.zeros((len(src_words),len(tgt_words)))\n",
        "    src_idx, tgt_idx = get_dictionary_index(filename, src_words, tgt_words, limit=limit)\n",
        "    F[np.asarray(src_idx), np.asarray(tgt_idx)] = 1\n",
        "    return F\n",
        "\n",
        "def get_data(src, tgt, top_n=200000):\n",
        "    src_arr, src_words = get_wordvec('./wiki.en.vec', top_n)\n",
        "    tgt_arr, tgt_words = get_wordvec('./wiki.bg.vec', top_n)\n",
        "    return src_arr, src_words, tgt_arr, tgt_words\n",
        "\n",
        "def sparsify_mat(K, nn):\n",
        "    ret_K = np.zeros(K.shape)\n",
        "    for i in range(K.shape[0]):\n",
        "        index = np.argsort(K[i, :])[-nn:]\n",
        "        ret_K[i, index] = K[i, index]\n",
        "    return ret_K\n",
        "\n",
        "def plot_coo_matrix(m):\n",
        "    if not isinstance(m, coo_matrix):\n",
        "        m = coo_matrix(m)\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, axisbg='black')\n",
        "    ax.plot(m.col, m.row, 's', color='white', ms=1)\n",
        "    ax.set_xlim(0, m.shape[1])\n",
        "    ax.set_ylim(0, m.shape[0])\n",
        "    ax.set_aspect('equal')\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    return ax\n",
        "\n",
        "def sym_sparsify_mat(K, nn):\n",
        "        K_sp = sparsify_mat(K, nn)\n",
        "        K_sp = (K_sp + K_sp.T) / 2  # in case of non-positive semi-definite\n",
        "        return K_sp\n",
        "\n",
        "def get_adj(basedir, src, tgt, nn, logger, normalize=True):\n",
        "    logger.info('Loading data...')\n",
        "    src_arr, tgt_arr = get_data(basedir, src, tgt)\n",
        "    logger.info('Loading data finished')\n",
        "    if normalize:\n",
        "        src_arr = src_arr / np.linalg.norm(src_arr, ord=2, axis=1, keepdims=True)\n",
        "        tgt_arr = tgt_arr / np.linalg.norm(tgt_arr, ord=2, axis=1, keepdims=True)\n",
        "    src_adj = sym_sparsify_mat(src_arr.dot(src_arr.T), nn)\n",
        "    tgt_adj = sym_sparsify_mat(tgt_arr.dot(tgt_arr.T), nn)\n",
        "    logger.info('Sparsification finished')\n",
        "    # print(type(src_adj), type(tgt_adj))\n",
        "    return torch.from_numpy(src_adj.astype(float)), torch.from_numpy(tgt_adj.astype(float))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n",
        "    basedir = \"./unsup-cross-lingual-embedding-transfer-master/data\"\n",
        "    src = 'es'\n",
        "    tgt = 'en'\n",
        "    nn = 5\n",
        "\n",
        "    src_arr, tgt_arr = get_data(basedir, src, tgt)\n",
        "\n",
        "    src_adj = sym_sparsify_mat(src_arr.dot(src_arr.T), nn)\n",
        "    tgt_adj = sym_sparsify_mat(tgt_arr.dot(tgt_arr.T), nn)\n",
        "    print(src_adj.shape)\n",
        "    print(tgt_adj.shape)\n",
        "    # print src_adj[0]\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-104e4f260b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0msrc_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0msrc_adj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msym_sparsify_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-104e4f260b39>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(src, tgt, top_n)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0msrc_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wordvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./wiki.en.vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0mtgt_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wordvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./wiki.bg.vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msrc_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-104e4f260b39>\u001b[0m in \u001b[0;36mget_wordvec\u001b[0;34m(filename, top_n)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtop_n\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'int' and 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNKtft05zNKl",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq-_9zIOzPCx",
        "colab_type": "text"
      },
      "source": [
        "Common_layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ubK_tgszM4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "USE_BN = True\n",
        "\n",
        "def highway_layer(input_layer, input_dim, hidden_dim, output_dim, Wg=None, bg=None, Ws=None, bs=None, name='highway'):\n",
        "    with tf.name_scope(name + \"-gate-layer\"):\n",
        "        if Wg is None:\n",
        "            Wg = tf.Variable(tf.truncated_normal([input_dim, input_dim], stddev=0.05), name=\"W\")\n",
        "        if bg is None:\n",
        "            bg = tf.Variable(tf.truncated_normal([input_dim], stddev=0.05), name=\"b\")\n",
        "        g = tf.nn.sigmoid(tf.add(tf.matmul(input_layer, Wg), bg))\n",
        "\n",
        "    trans, Ws, bs = dense_layers(input_layer, input_dim, hidden_dim, output_dim, Ws=Ws, bs=bs, name=name+'-dense-layer')\n",
        "    out = tf.multiply(g, trans) + tf.multiply(1-g, input_layer)\n",
        "    return out, Wg, bg, Ws, bs\n",
        "\n",
        "def dense_layers(input_layer, input_dim, hidden_dim, output_dim, Ws=None, bs=None, name='fc', activation='elu', bias=True, orth_init=False, add_multiple_noise=None, BN=USE_BN, BN_phase=True, BN_reuse=None):\n",
        "    if Ws is None:\n",
        "        Ws = (len(hidden_dim)+1)*[None]\n",
        "    if bs is None:\n",
        "        bs = (len(hidden_dim)+1)*[None]\n",
        "    dims = [input_dim] + hidden_dim + [output_dim]\n",
        "    layers = [input_layer] + len(hidden_dim)*[None] + [None]\n",
        "    with tf.name_scope(name):\n",
        "        for i, d_i in enumerate(dims[:-1]):\n",
        "            # add multiple noise to input layers\n",
        "            if add_multiple_noise is not None:\n",
        "                # print(\"add_multiple_noise\", add_multiple_noise)\n",
        "                gaussian_noise = tf.random_normal(shape = tf.shape(layers[i]), mean = 1.0, stddev = add_multiple_noise, dtype = tf.float32)\n",
        "                layers[i] = tf.multiply(layers[i], gaussian_noise)\n",
        "            # Wx + b\n",
        "            d_o = dims[i+1]\n",
        "            if Ws[i] is None:\n",
        "                if orth_init:\n",
        "                    initializer = tf.orthogonal_initializer\n",
        "                else:\n",
        "                    initializer = tf.contrib.layers.xavier_initializer()\n",
        "                Ws[i] = tf.get_variable(name=\"W\"+str(i)+name, shape=[d_i, d_o], dtype=tf.float32, initializer=initializer)\n",
        "            if bs[i] is None and bias:\n",
        "                bs[i] = tf.Variable(tf.constant(0.1, shape=[d_o], dtype=tf.float32), name=\"b\"+str(i))\n",
        "            elif not bias:\n",
        "                bs[i] = tf.constant(0, shape=[d_o], dtype=tf.float32)\n",
        "\n",
        "            layers[i+1] = tf.nn.xw_plus_b(layers[i], Ws[i], bs[i], name=\"fc_out\"+str(i+1))\n",
        "\n",
        "            if BN and i!=len(hidden_dim): # add BN\n",
        "            # if BN:\n",
        "                layers[i+1] = tf.contrib.layers.batch_norm(layers[i+1], center=True, scale=True, is_training=BN_phase, scope=name+'_bn_'+str(i), reuse=BN_reuse)\n",
        "\n",
        "            if i!=len(hidden_dim) and activation:\n",
        "                # layers[i+1] = tf.nn.tanh(layers[i+1]) # tanh activation for any other output\n",
        "                if activation == 'elu':\n",
        "                    layers[i+1] = tf.nn.elu(layers[i+1]) # elu activation for any other output \n",
        "                elif activation == 'relu':\n",
        "                    layers[i+1] = tf.nn.relu(layers[i+1]) # relu activation for any other output\n",
        "                elif activation == 'tanh':\n",
        "                    layers[i+1] = tf.nn.tanh(layers[i+1]) # tanh activation for any other output\n",
        "\n",
        "    return layers[len(hidden_dim)+1], Ws, bs \n",
        "\n",
        "def add_multiple_gaussian_noise(inp_tensor, stddev):\n",
        "    gaussian_noise = tf.random_normal(shape = tf.shape(inp_tensor), mean = 1.0, stddev = stddev, dtype = tf.float32)\n",
        "    return tf.multiply(inp_tensor, gaussian_noise)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnsZrW1tznZ2",
        "colab_type": "text"
      },
      "source": [
        "Cycle_Align_Emb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47eirPigzrGo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "38b0b42e-fc25-4f09-8ca7-9c27a1bf3bdf"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import print_function\n",
        "import os, sys\n",
        "import numpy as np\n",
        "#sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
        "from tqdm import tqdm\n",
        "import errno\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.layers as ly\n",
        "from tensorflow.python import debug as tf_debug\n",
        "\n",
        "from utils.callbacks import SGDWR, EarlyStopper, ModelChecker\n",
        "from utils.eval_helper import knn_accuracy_from_list, find_common_words\n",
        "from utils.visual_emb import plot_align_matrix, plot_emb\n",
        "from utils.data_helper import load_text_vec, load_freq, uniform_batch_iter, freq_sample_batch_iter, sparsify_mat, mkdir_p\n",
        "\n",
        "from model.distance import *\n",
        "from model.common_layers import *\n",
        "from model.cycle_emb import CycleEmb, USE_EUC_DIST, TIED_WEIGHTS\n",
        "from model.sinkhorn import get_sinkhorn_distance\n",
        "from model.wasserstein_gan import set_W_gan_layers\n",
        "\n",
        "from numpy import linalg as LA\n",
        "from functools import partial\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "USE_EUC_DIST_MATRIX = False\n",
        "TRANS_ITER = 1\n",
        "DISC_ITERS = 5 # for WGAN, we iterate discriminator\n",
        "WGAN_TRANS_ITERS = 5 # for WGAN, we iterate transformation\n",
        "DEBUG = False\n",
        "LAMBDA_SH = 10\n",
        "SINKHORN_LAYER_DEPTH = 20\n",
        "MULTIPLE_NOISE_STD = None # add noise to sinkhorn layers\n",
        "PATIENCE = 4000 # number of epochs we wait before call early stop\n",
        "MIN_DELTA = 1e-5 # improvement larger than delta is considered as improvement\n",
        "\n",
        "################################################################################\n",
        "### The cross-lingual embeding model learned from cycle adversarial network\n",
        "class CycleAlignEmb(CycleEmb):\n",
        "    def __init__(self, trans_hidden_dim=[50], disc_hidden_dim=[50], recon_loss_weight=[1,1], constraint_loss_weight=1, init_align_loss_weight=1, trans_activation=False, lambda_sh=10, sinkhorn_layer_depth=20, cyc_loss=True, use_sinkhorn=True, use_BN=True, norm='scale', save_path=None, F_init=None):\n",
        "        CycleEmb.__init__(self, trans_hidden_dim, disc_hidden_dim, recon_loss_weight, trans_activation, cyc_loss, use_sinkhorn, use_BN, norm, save_path)\n",
        "        self.constraint_loss_weight = constraint_loss_weight # relative weight of loss of F constraint\n",
        "        self.init_align_loss_weight = init_align_loss_weight # if we have initial seed, the weight of that part of loss\n",
        "        self.lambda_sh = lambda_sh\n",
        "        self.sinkhorn_layer_depth = sinkhorn_layer_depth\n",
        "        self.F_init = F_init\n",
        "        self.cyc_loss = cyc_loss\n",
        "\n",
        "    def build_WGAN_layer(self, ):\n",
        "        if self.F_init is None:\n",
        "            l_disc_s, l_gen_s, self.Ws_s_vs_fs, self.bs_s_vs_fs = set_W_gan_layers(self.input_s, self.trans_t, input_dim=self.emb_dim_list[0], disc_hidden_dim=self.disc_hidden_dim, scale=10, name='W_gan_s')\n",
        "            l_disc_t, l_gen_t, self.Ws_t_vs_ft, self.bs_t_vs_ft = set_W_gan_layers(self.input_t, self.trans_s, input_dim=self.emb_dim_list[1], disc_hidden_dim=self.disc_hidden_dim, scale=10, name='W_gan_t')\n",
        "            if self.cyc_loss:\n",
        "                self.l_disc = (l_disc_s + l_disc_t)/2\n",
        "                self.l_gen = (l_gen_s + l_gen_t)/2\n",
        "            else:\n",
        "                self.l_disc = l_disc_s\n",
        "                self.l_gen = l_gen_s\n",
        "        else:\n",
        "            self.l_disc = tf.constant(0, dtype=tf.float32)\n",
        "            self.l_gen = tf.constant(0, dtype=tf.float32)\n",
        "\n",
        "    def build_sinkhorn_layer(self, lambda_sh=LAMBDA_SH, sinkhorn_layer_depth=SINKHORN_LAYER_DEPTH, multiple_noise_std=None):\n",
        "        '''\n",
        "        Following \"Marco Cuturi, Sinkhorn Distances: Lightspeed Computation of Optimal Transport, NIPS 2013\"\n",
        "        '''\n",
        "        if multiple_noise_std is not None:\n",
        "            self.input_t_inp = add_multiple_gaussian_noise(self.input_t, multiple_noise_std)\n",
        "            self.trans_s_inp = add_multiple_gaussian_noise(self.trans_s, multiple_noise_std)\n",
        "            self.trans_t_inp = add_multiple_gaussian_noise(self.trans_t, multiple_noise_std)\n",
        "            self.input_s_inp = add_multiple_gaussian_noise(self.input_s, multiple_noise_std)\n",
        "        else:\n",
        "            self.input_t_inp = self.input_t\n",
        "            self.trans_s_inp = self.trans_s\n",
        "            self.trans_t_inp = self.trans_t\n",
        "            self.input_s_inp = self.input_s\n",
        "\n",
        "        M1 = euclidean_distance_matrix(self.trans_s_inp, self.input_t_inp) if USE_EUC_DIST_MATRIX else norm_euclidean_distance_matrix(self.trans_s_inp, self.input_t_inp)\n",
        "        M2 = euclidean_distance_matrix(self.input_s_inp, self.trans_t_inp) if USE_EUC_DIST_MATRIX else norm_euclidean_distance_matrix(self.input_s_inp, self.trans_t_inp)\n",
        "        \n",
        "        if self.cyc_loss:\n",
        "            self.sinkhorn_distance = (get_sinkhorn_distance(M1, self.freq_s, self.freq_t, lambda_sh=lambda_sh, depth=sinkhorn_layer_depth) + \\\n",
        "                                  get_sinkhorn_distance(M2, self.freq_s, self.freq_t, lambda_sh=lambda_sh, depth=sinkhorn_layer_depth))/2\n",
        "        else:\n",
        "            self.sinkhorn_distance = get_sinkhorn_distance(M2, self.freq_s, self.freq_t, lambda_sh=lambda_sh, depth=sinkhorn_layer_depth)\n",
        "\n",
        "    def build_F_init_layer(self):\n",
        "        if self.F_init is not None:\n",
        "            t_nz_idx, s_nz_idx = np.where(self.F_init>0)\n",
        "            t_nz_idx, s_nz_idx = np.sort(t_nz_idx), np.sort(s_nz_idx)\n",
        "            self.F_init = self.F_init[t_nz_idx,:]\n",
        "            self.F_init = self.F_init[:,s_nz_idx]\n",
        "            self.F_init_row=self.F_init/np.sum(self.F_init, axis=1, keepdims=True)\n",
        "            self.F_init_col=np.transpose(self.F_init/np.sum(self.F_init, axis=0, keepdims=True))\n",
        "            # print('self.F_init', self.F_init)\n",
        "            # print('self.F_init.shape', self.F_init.shape)\n",
        "            # take care of the initial dictionary\n",
        "            # F_init_count = np.count_nonzero(self.F_init)\n",
        "            # print('F_init_count', F_init_count)\n",
        "            F_init_row = tf.convert_to_tensor(self.F_init_row, dtype=tf.float32)\n",
        "            F_init_col = tf.convert_to_tensor(self.F_init_col, dtype=tf.float32)\n",
        "\n",
        "            self.init_vecs_list = [None, None]\n",
        "            self.init_vecs_list[0] = self.vecs_list[0][s_nz_idx,:]\n",
        "            self.init_vecs_list[1] = self.vecs_list[1][t_nz_idx,:]\n",
        "\n",
        "            if USE_EUC_DIST:\n",
        "                self.align_loss_row_init = euclidean_distance(self.trans_s_init, self.input_t_init) / tf.cast(self.F_init.shape[0], dtype=tf.float32)\n",
        "                self.align_loss_col_init = euclidean_distance(self.trans_t_init, self.input_s_init) / tf.cast(self.F_init.shape[1], dtype=tf.float32)\n",
        "            else:\n",
        "                self.align_loss_row_init = 1 - cosine_similarity(tf.matmul(F_init_row, self.trans_s_init), self.input_t_init) / tf.cast(self.F_init.shape[0], dtype=tf.float32)\n",
        "                self.align_loss_col_init = 1 - cosine_similarity(tf.matmul(F_init_col, self.trans_t_init), self.input_s_init) / tf.cast(self.F_init.shape[1], dtype=tf.float32)\n",
        "            self.init_align_loss = (self.align_loss_row_init + self.align_loss_col_init)/2\n",
        "        else:\n",
        "            self.init_align_loss = tf.constant(0, dtype=tf.float32)\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\" \n",
        "        build the graph for training\n",
        "        internal dimentions:\n",
        "        bs: batch_size, ed: embedding dimension, hd: hidden dimension\n",
        "        \"\"\"\n",
        "        self.build_input_layers()\n",
        "        self.build_transfer_layers()\n",
        "\n",
        "        # alignment loss\n",
        "        self.build_sinkhorn_layer(lambda_sh=self.lambda_sh, sinkhorn_layer_depth=self.sinkhorn_layer_depth, multiple_noise_std=MULTIPLE_NOISE_STD)\n",
        "        self.build_F_init_layer()\n",
        "        self.build_WGAN_layer()\n",
        "\n",
        "        # recontruction loss\n",
        "        self.build_recontruction_layers()\n",
        "        self.l_trans_wgan = self.l_gen + self.recon_loss_weight[0]*self.reconstruct_loss\n",
        "        self.l_trans_sh = self.sinkhorn_distance + self.recon_loss_weight[1]*self.reconstruct_loss + self.init_align_loss_weight*self.init_align_loss\n",
        "\n",
        "        self.set_summary()\n",
        "\n",
        "    def set_summary(self):\n",
        "        tf.summary.scalar('discriminator loss', self.l_disc)\n",
        "        tf.summary.scalar('wgan transformation(generator) loss', self.l_trans_wgan)\n",
        "        tf.summary.scalar('sinkhorn transformation loss', self.l_trans_sh)\n",
        "        tf.summary.scalar('reconstruct loss', self.reconstruct_loss)\n",
        "        tf.summary.scalar('init alignment loss', self.init_align_loss)\n",
        "        tf.summary.scalar('sinkhorn distance', self.sinkhorn_distance)\n",
        "        if len(self.trans_hidden_dim) == 0:\n",
        "            W = self.Ws_s2t[0]\n",
        "            self.orth_loss = tf.norm(tf.matmul(W, W, transpose_a=True)-tf.eye(self.emb_dim_list[0]), ord='fro', axis=(0,1))\n",
        "            tf.summary.scalar('orth_loss', self.orth_loss)\n",
        "        self.merged_summary = tf.summary.merge_all()\n",
        "\n",
        "    def train(self, batch_size=128, lr=0.001, epoch=10000, wgan_epoch=2000, logger=None, validation=None, dp_keep_prob=1.0, patience=PATIENCE, min_delta = MIN_DELTA):\n",
        "        \"\"\" \n",
        "        train with adversarial/sinkhorn loss\n",
        "        log: log file to record training\n",
        "        validation: [tgt_words, true_src_words]\n",
        "        \"\"\"\n",
        "        batch_per_epoch = int(self.vecs_list[0].shape[0]/batch_size) + 1 # how many steps in a epoch\n",
        "        total_steps = batch_per_epoch * epoch\n",
        "        early_stopper = EarlyStopper(patience*batch_per_epoch, min_delta)\n",
        "        model_checker = ModelChecker()\n",
        "\n",
        "        learning_rate = tf.placeholder(tf.float32, shape=[])\n",
        "        lr_schedule_dict = SGDWR(T_total=epoch, T_0=10, T_mult=1.2, lr_max=lr, lr_min=lr/100)\n",
        "\n",
        "        if self.F_init is None: # we will switch from WGAN to Sinkhorn during training\n",
        "            swith_flag = False # first WGAN and then Sinkhorn\n",
        "            # swith_flag = True # Use Sinkhorn all the time\n",
        "\n",
        "        with tf.Session(config=self.session_conf) as sess:\n",
        "            # sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
        "            # sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
        "            self.writer = tf.summary.FileWriter(os.path.join(self.save_path, 'graph'), graph=sess.graph)\n",
        "            if self.saver is None:\n",
        "                self.saver = tf.train.Saver(tf.global_variables())\n",
        "                # self.saver = tf.train.Saver(trans_variables)\n",
        "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "            \n",
        "            # define train operation\n",
        "            if TIED_WEIGHTS:\n",
        "                trans_variables = self.Ws_s2t\n",
        "            else:\n",
        "                trans_variables = self.Ws_s2t + self.Ws_t2s\n",
        "            if self.F_init is None:\n",
        "                disc_variables = self.Ws_s_vs_fs + self.bs_s_vs_fs + self.Ws_t_vs_ft + self.bs_t_vs_ft\n",
        "\n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "            with tf.control_dependencies(update_ops):\n",
        "                sh_trans_optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "                # cliping the weight norm\n",
        "                # grads_and_vars = sh_trans_optimizer.compute_gradients(self.l_trans_sh)\n",
        "                # capped_grads_and_vars = [(tf.clip_by_norm(gv[0], clip_norm=10, axes=0), gv[1])\n",
        "                #              for gv in grads_and_vars]\n",
        "                # sh_trans_optimizer = sh_trans_optimizer.apply_gradients(capped_grads_and_vars)\n",
        "                sh_trans_train_op = sh_trans_optimizer.minimize(self.l_trans_sh, global_step=global_step, var_list=trans_variables)\n",
        "\n",
        "                if self.F_init is None:\n",
        "                    wgan_trans_optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate*10)\n",
        "                    wgan_trans_train_op = wgan_trans_optimizer.minimize(self.l_trans_wgan, global_step=global_step, var_list=trans_variables)\n",
        "                    disc_optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "                    disc_train_op = disc_optimizer.minimize(self.l_disc, var_list=disc_variables)\n",
        "\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            step = 0\n",
        "\n",
        "            batches_src = uniform_batch_iter([self.vecs_list[0], self.freq_list[0]], batch_size=batch_size, num_epochs=epoch, shuffle=True, full_batch=True, verbose=False)\n",
        "            batches_tgt = uniform_batch_iter([self.vecs_list[1], self.freq_list[1]], batch_size=batch_size, num_epochs=epoch, shuffle=True, full_batch=True, verbose=False)\n",
        "            val_acc = 0.0\n",
        "            cur_lr = lr\n",
        "            epoch_start_time = time.time()\n",
        "\n",
        "            for batch_src, batch_tgt in zip(batches_src, batches_tgt):\n",
        "                x_src_batch = batch_src[0][0]\n",
        "                x_tgt_batch = batch_tgt[0][0]\n",
        "                freq_src_batch = batch_src[0][1]\n",
        "                freq_tgt_batch = batch_tgt[0][1]\n",
        "\n",
        "                # more advenced lr scheduler\n",
        "                # T = int(step/batch_per_epoch) + 1\n",
        "                # cur_lr = lr_schedule_dict[T]    \n",
        "                '''OR'''\n",
        "                # if step < total_steps/2:\n",
        "                #     cur_lr = lr\n",
        "                # else:\n",
        "                #     cur_lr = lr/10\n",
        "                '''OR'''\n",
        "                # simplest lr scheduler\n",
        "                # cur_lr = lr\n",
        "\n",
        "                feed_dict = {\n",
        "                  self.input_s: x_src_batch,\n",
        "                  self.input_t: x_tgt_batch,\n",
        "                  self.dp_keep_prob: dp_keep_prob,\n",
        "                  self.freq_s: freq_src_batch,\n",
        "                  self.freq_t: freq_tgt_batch,\n",
        "                  learning_rate: cur_lr,\n",
        "                  self.phase: True,\n",
        "                }\n",
        "                if self.F_init is not None:\n",
        "                    feed_dict[self.input_s_init] = self.init_vecs_list[0]\n",
        "                    feed_dict[self.input_t_init] = self.init_vecs_list[1]\n",
        "\n",
        "                if self.F_init is None and (not swith_flag): \n",
        "                    \n",
        "                    # update WGAN disc models\n",
        "                    for _ in range(DISC_ITERS):\n",
        "                        _, l_disc, l_trans_wgan = sess.run([disc_train_op, self.l_disc, self.l_trans_wgan], feed_dict)\n",
        "                    # print(\"l_trans_wgan before wgan_trans_train_op\", l_trans_wgan)\n",
        "\n",
        "                    # update WGAN transfer models\n",
        "                    # for _ in range(WGAN_TRANS_ITERS):\n",
        "                    _, step, loss_reconstruct, loss_comb, l_trans_wgan, summary = sess.run([wgan_trans_train_op, global_step, self.reconstruct_loss, self.sinkhorn_distance, self.l_trans_wgan, self.merged_summary], feed_dict)\n",
        "                    # print(\"l_trans_wgan after wgan_trans_train_op\", l_trans_wgan)\n",
        "\n",
        "                    # judge convergence, if so, switch training loss\n",
        "                    # if early_stopper.check_early_stop(loss_comb, int( (step-0.1) / batch_per_epoch)+1) or step > total_steps/2:\n",
        "                    if step > wgan_epoch * batch_per_epoch:\n",
        "                        swith_flag = True\n",
        "                        if not self.use_sinkhorn:\n",
        "                            logger.info('Epoch, {0}, end up just using WGAN!'.format(int( (step-0.1) / batch_per_epoch)+1))\n",
        "                            break\n",
        "                        logger.info('Epoch, {0}, switch to sinkhorn loss!'.format(int( (step-0.1) / batch_per_epoch)+1))\n",
        "                        self.saver.restore(sess, os.path.join(self.save_path, 'model.ckpt'))\n",
        "                        early_stopper = EarlyStopper(patience*batch_per_epoch, min_delta)\n",
        "                        model_checker = ModelChecker()\n",
        "                        cur_lr = lr\n",
        "                elif self.use_sinkhorn: # update sinkhorn transfer models\n",
        "                    for _ in range(TRANS_ITER):\n",
        "                        _, step, loss_reconstruct, loss_comb, init_align_loss, summary = sess.run([sh_trans_train_op, global_step, self.reconstruct_loss, self.l_trans_sh, self.init_align_loss, self.merged_summary], feed_dict)\n",
        "\n",
        "                    # handle early stopping\n",
        "                    # if step % batch_per_epoch == 0:\n",
        "                    if early_stopper.check_early_stop(loss_comb, int( (step-0.1) / batch_per_epoch)+1):\n",
        "                        logger.info('Epoch, {0} early stopping!'.format(int( (step-0.1) / batch_per_epoch)+1))\n",
        "                        print('Epoch, {0} early stopping!'.format(int( (step-0.1) / batch_per_epoch)+1))\n",
        "                        break\n",
        "                else:\n",
        "                    print('ERROR: Invalid config for training!')\n",
        "\n",
        "                # validation and print\n",
        "                if step % (50*batch_per_epoch) == 0:\n",
        "                    validation_start_time = time.time()\n",
        "                    stdout_buffer = ''\n",
        "                    self.writer.add_summary(summary, step)\n",
        "                    stdout_buffer += 'VALIDATION: Epoch:{} '.format(int( (step-0.1) / batch_per_epoch)+1)\n",
        "\n",
        "                    if DEBUG:\n",
        "                        for w in ['城市', '小行星', '文学']:\n",
        "                            w_knn = self.find_k_nearest(w=w, k=5, src2tgt=True)\n",
        "                            print('word: %s has knn words --- %s' % (w, ' '.join(w_knn)))\n",
        "                    if validation is not None:\n",
        "                        self.transfer_tgt_emb(sess=sess)\n",
        "                        # print('validation[0][:10]:', validation[0][:10])\n",
        "                        # print('validation[1][:10]:', validation[1][:10])\n",
        "                        nnr_tgt = self.find_nearest_gpu(validation[0], src2tgt=True, batch_size=batch_size, sess=sess)\n",
        "                        # print('nnr_tgt[:10]:', nnr_tgt[:10])\n",
        "                        val_acc = knn_accuracy_from_list(nnr_tgt, validation[1], k=1)\n",
        "                        stdout_buffer += 'Bilex acc:{0:.4f} '.format(val_acc)\n",
        "                        val_acc_value = tf.Summary.Value(tag='val_acc', simple_value=val_acc)\n",
        "                        self.writer.add_summary(tf.Summary(value=[val_acc_value]), step)\n",
        "\n",
        "                    stdout_buffer += 'Validation eclipsed time \\n'.format(time.time()-validation_start_time)\n",
        "                    sys.stdout.write(stdout_buffer)\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "                # check objective to save the best model\n",
        "                model_checker.record_loss(loss_comb, int( (step-0.1) / batch_per_epoch)+1)\n",
        "                if step % batch_per_epoch == 0:\n",
        "                    print('Epoch  {}, time eclipsed {}'.format(int((step-0.1) / batch_per_epoch)+1, time.time()-epoch_start_time))\n",
        "                    epoch_start_time = time.time()\n",
        "                    if model_checker.check_for_best(loss_comb, int((step-0.1) / batch_per_epoch)+1):\n",
        "                        save_path = self.saver.save(sess, os.path.join(self.save_path, 'model.ckpt'))\n",
        "                        best_loss = model_checker.get_best_loss()\n",
        "                        print('Saved at combined loss {}'.format(best_loss))\n",
        "                        if validation is not None and logger is not None:\n",
        "                            logger.info('Saved at epoch:{0}, step:{1} loss:{2:.4f} Bilingual Induction Accuracy --- {3:.4f}'.format(int( (step-0.1) / batch_per_epoch)+1, step, best_loss, val_acc))\n",
        "                        cur_lr = lr\n",
        "                    else:\n",
        "                        cur_lr = lr*0.95\n",
        "\n",
        "        self.writer.close()\n",
        "        return val_acc"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c3f4e4993b75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEarlyStopper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mknn_accuracy_from_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_common_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_emb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_align_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}