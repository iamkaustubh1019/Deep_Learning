{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamkaustubh1019/Deep_Learning/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub6b1kCpjdOv",
        "colab_type": "code",
        "outputId": "0cf9b4cd-b9d6-4b38-9330-ceda195a0a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S25gVvzK_jQ7",
        "colab_type": "code",
        "outputId": "470933db-4f10-484f-9dda-06dca4d64ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install numpy==1.16.1\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVh3WHvN_4_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lemiV2ApALt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imdb = keras.datasets.imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg7fdcI5CLdM",
        "colab_type": "code",
        "outputId": "c8880a46-339b-406e-d397-39b198d23e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training entries: 25000, labels: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T-AQeloCRDs",
        "colab_type": "code",
        "outputId": "bcf5aed9-b0d6-44f0-fc02-8f414f82f422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN0PQVel_kYZ",
        "colab_type": "code",
        "outputId": "720b80c0-99e2-43c3-e134-87631c90868e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(218, 189)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3zQRwlICygC",
        "colab_type": "code",
        "outputId": "6fdd39b2-ad99-49fb-bb49-1fabbb351199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} \n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-IbuGiKCwkN",
        "colab_type": "code",
        "outputId": "047b456f-e4ef-473a-8b27-d2b877712388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "decode_review(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHhQTfQRjhwz",
        "colab_type": "code",
        "outputId": "9609a2a8-49fc-4688-fd07-163b170cf065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACmBOF2U3D-y",
        "colab_type": "code",
        "outputId": "ef49bc86-e70c-49ad-df76-95619da0f119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e47N3Ato3Iya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dhZ_W11kVd_",
        "colab_type": "code",
        "outputId": "103c85d5-ec1a-4351-fcd0-5775f8da3b74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "\n",
        "text = nltk.word_tokenize(\"Time flies like an arrow\")\n",
        "text2 = nltk.word_tokenize(\"Your efforts will bear fruits\")\n",
        "nltk.pos_tag(text)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Time', 'NNP'),\n",
              " ('flies', 'NNS'),\n",
              " ('like', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('arrow', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2sT3SUl7qlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUDp72wH7hFJ",
        "colab_type": "code",
        "outputId": "8e301c19-b1c3-45b3-faf4-0c3ec66f7370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "nltk.pos_tag(text2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Your', 'PRP$'),\n",
              " ('efforts', 'NNS'),\n",
              " ('will', 'MD'),\n",
              " ('bear', 'VB'),\n",
              " ('fruits', 'NNS')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCqNLY0M7ryH",
        "colab_type": "code",
        "outputId": "ffa9dfd2-6791-40ce-d288-a2070b99d79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3ab6337502f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_mini_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWp47ouq7if_",
        "colab_type": "code",
        "outputId": "1618c8a7-6ea6-4dc2-cc50-f35ed8ee66ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " import numpy as np\n",
        " import pandas as pd\n",
        " from keras.models import Sequential\n",
        " from keras.layers import Dense,Activation,Layer,Lambda "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-13G_TL53Kqe",
        "colab_type": "code",
        "outputId": "a34b6076-202e-4c5b-934c-a88c27cb89a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dataset = pd.read_csv(\"https://github.com/iamkaustubh1019/Movie_recommendation_system_Machinelearning/blob/master/data.csv\",error_bad_lines=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 75: expected 1 fields, saw 5\\nSkipping line 124: expected 1 fields, saw 3\\nSkipping line 139: expected 1 fields, saw 11\\nSkipping line 162: expected 1 fields, saw 4\\nSkipping line 166: expected 1 fields, saw 2\\nSkipping line 168: expected 1 fields, saw 2\\nSkipping line 169: expected 1 fields, saw 2\\nSkipping line 170: expected 1 fields, saw 2\\nSkipping line 171: expected 1 fields, saw 2\\nSkipping line 172: expected 1 fields, saw 2\\nSkipping line 173: expected 1 fields, saw 2\\nSkipping line 174: expected 1 fields, saw 2\\nSkipping line 175: expected 1 fields, saw 2\\nSkipping line 176: expected 1 fields, saw 2\\nSkipping line 180: expected 1 fields, saw 2\\nSkipping line 181: expected 1 fields, saw 2\\nSkipping line 187: expected 1 fields, saw 2\\nSkipping line 195: expected 1 fields, saw 4\\nSkipping line 201: expected 1 fields, saw 2\\nSkipping line 206: expected 1 fields, saw 2\\nSkipping line 207: expected 1 fields, saw 2\\nSkipping line 208: expected 1 fields, saw 2\\nSkipping line 209: expected 1 fields, saw 2\\nSkipping line 210: expected 1 fields, saw 2\\nSkipping line 215: expected 1 fields, saw 2\\nSkipping line 216: expected 1 fields, saw 2\\nSkipping line 217: expected 1 fields, saw 2\\nSkipping line 224: expected 1 fields, saw 2\\nSkipping line 232: expected 1 fields, saw 4\\nSkipping line 237: expected 1 fields, saw 2\\nSkipping line 240: expected 1 fields, saw 2\\nSkipping line 241: expected 1 fields, saw 2\\nSkipping line 245: expected 1 fields, saw 2\\nSkipping line 246: expected 1 fields, saw 2\\nSkipping line 268: expected 1 fields, saw 2\\nSkipping line 403: expected 1 fields, saw 11\\nSkipping line 405: expected 1 fields, saw 11\\nSkipping line 446: expected 1 fields, saw 9\\nSkipping line 457: expected 1 fields, saw 9\\nSkipping line 469: expected 1 fields, saw 9\\nSkipping line 595: expected 1 fields, saw 3\\nSkipping line 599: expected 1 fields, saw 3\\nSkipping line 600: expected 1 fields, saw 11\\nSkipping line 668: expected 1 fields, saw 2\\nSkipping line 735: expected 1 fields, saw 2\\nSkipping line 743: expected 1 fields, saw 33\\nSkipping line 748: expected 1 fields, saw 32\\nSkipping line 753: expected 1 fields, saw 32\\nSkipping line 758: expected 1 fields, saw 32\\nSkipping line 763: expected 1 fields, saw 32\\nSkipping line 768: expected 1 fields, saw 32\\nSkipping line 773: expected 1 fields, saw 32\\nSkipping line 778: expected 1 fields, saw 32\\nSkipping line 783: expected 1 fields, saw 32\\nSkipping line 788: expected 1 fields, saw 32\\nSkipping line 793: expected 1 fields, saw 32\\nSkipping line 798: expected 1 fields, saw 32\\nSkipping line 803: expected 1 fields, saw 32\\nSkipping line 808: expected 1 fields, saw 32\\nSkipping line 813: expected 1 fields, saw 32\\nSkipping line 818: expected 1 fields, saw 32\\nSkipping line 823: expected 1 fields, saw 32\\nSkipping line 828: expected 1 fields, saw 32\\nSkipping line 833: expected 1 fields, saw 32\\nSkipping line 838: expected 1 fields, saw 32\\nSkipping line 843: expected 1 fields, saw 32\\nSkipping line 848: expected 1 fields, saw 32\\nSkipping line 853: expected 1 fields, saw 32\\nSkipping line 858: expected 1 fields, saw 32\\nSkipping line 863: expected 1 fields, saw 32\\nSkipping line 868: expected 1 fields, saw 32\\nSkipping line 873: expected 1 fields, saw 32\\nSkipping line 878: expected 1 fields, saw 32\\nSkipping line 883: expected 1 fields, saw 32\\nSkipping line 888: expected 1 fields, saw 32\\nSkipping line 893: expected 1 fields, saw 32\\nSkipping line 898: expected 1 fields, saw 32\\nSkipping line 903: expected 1 fields, saw 32\\nSkipping line 908: expected 1 fields, saw 32\\nSkipping line 913: expected 1 fields, saw 32\\nSkipping line 918: expected 1 fields, saw 32\\nSkipping line 923: expected 1 fields, saw 32\\nSkipping line 928: expected 1 fields, saw 32\\nSkipping line 933: expected 1 fields, saw 32\\nSkipping line 938: expected 1 fields, saw 32\\nSkipping line 943: expected 1 fields, saw 32\\nSkipping line 948: expected 1 fields, saw 32\\nSkipping line 953: expected 1 fields, saw 32\\nSkipping line 958: expected 1 fields, saw 32\\nSkipping line 963: expected 1 fields, saw 32\\nSkipping line 968: expected 1 fields, saw 32\\nSkipping line 973: expected 1 fields, saw 32\\nSkipping line 978: expected 1 fields, saw 32\\nSkipping line 983: expected 1 fields, saw 32\\nSkipping line 988: expected 1 fields, saw 32\\nSkipping line 993: expected 1 fields, saw 32\\nSkipping line 998: expected 1 fields, saw 32\\nSkipping line 1003: expected 1 fields, saw 32\\nSkipping line 1008: expected 1 fields, saw 32\\nSkipping line 1013: expected 1 fields, saw 32\\nSkipping line 1018: expected 1 fields, saw 32\\nSkipping line 1023: expected 1 fields, saw 32\\nSkipping line 1028: expected 1 fields, saw 32\\nSkipping line 1033: expected 1 fields, saw 32\\nSkipping line 1038: expected 1 fields, saw 32\\nSkipping line 1043: expected 1 fields, saw 32\\nSkipping line 1048: expected 1 fields, saw 32\\nSkipping line 1053: expected 1 fields, saw 32\\nSkipping line 1058: expected 1 fields, saw 32\\nSkipping line 1063: expected 1 fields, saw 32\\nSkipping line 1068: expected 1 fields, saw 32\\nSkipping line 1073: expected 1 fields, saw 32\\nSkipping line 1078: expected 1 fields, saw 32\\nSkipping line 1083: expected 1 fields, saw 32\\nSkipping line 1088: expected 1 fields, saw 32\\nSkipping line 1093: expected 1 fields, saw 32\\nSkipping line 1098: expected 1 fields, saw 32\\nSkipping line 1103: expected 1 fields, saw 32\\nSkipping line 1108: expected 1 fields, saw 32\\nSkipping line 1113: expected 1 fields, saw 32\\nSkipping line 1118: expected 1 fields, saw 32\\nSkipping line 1123: expected 1 fields, saw 32\\nSkipping line 1128: expected 1 fields, saw 32\\nSkipping line 1133: expected 1 fields, saw 32\\nSkipping line 1138: expected 1 fields, saw 32\\nSkipping line 1143: expected 1 fields, saw 32\\nSkipping line 1148: expected 1 fields, saw 32\\nSkipping line 1153: expected 1 fields, saw 32\\nSkipping line 1158: expected 1 fields, saw 32\\nSkipping line 1163: expected 1 fields, saw 32\\nSkipping line 1168: expected 1 fields, saw 32\\nSkipping line 1173: expected 1 fields, saw 32\\nSkipping line 1178: expected 1 fields, saw 32\\nSkipping line 1183: expected 1 fields, saw 32\\nSkipping line 1188: expected 1 fields, saw 32\\nSkipping line 1193: expected 1 fields, saw 32\\nSkipping line 1198: expected 1 fields, saw 32\\nSkipping line 1203: expected 1 fields, saw 32\\nSkipping line 1208: expected 1 fields, saw 32\\nSkipping line 1213: expected 1 fields, saw 32\\nSkipping line 1218: expected 1 fields, saw 32\\nSkipping line 1223: expected 1 fields, saw 32\\nSkipping line 1228: expected 1 fields, saw 32\\nSkipping line 1233: expected 1 fields, saw 32\\nSkipping line 1238: expected 1 fields, saw 32\\nSkipping line 1243: expected 1 fields, saw 32\\nSkipping line 1248: expected 1 fields, saw 32\\nSkipping line 1253: expected 1 fields, saw 32\\nSkipping line 1258: expected 1 fields, saw 32\\nSkipping line 1263: expected 1 fields, saw 32\\nSkipping line 1268: expected 1 fields, saw 32\\nSkipping line 1273: expected 1 fields, saw 32\\nSkipping line 1278: expected 1 fields, saw 32\\nSkipping line 1283: expected 1 fields, saw 32\\nSkipping line 1288: expected 1 fields, saw 32\\nSkipping line 1293: expected 1 fields, saw 32\\nSkipping line 1298: expected 1 fields, saw 32\\nSkipping line 1303: expected 1 fields, saw 32\\nSkipping line 1308: expected 1 fields, saw 32\\nSkipping line 1313: expected 1 fields, saw 32\\nSkipping line 1318: expected 1 fields, saw 32\\nSkipping line 1323: expected 1 fields, saw 32\\nSkipping line 1328: expected 1 fields, saw 32\\nSkipping line 1333: expected 1 fields, saw 32\\nSkipping line 1338: expected 1 fields, saw 32\\nSkipping line 1343: expected 1 fields, saw 32\\nSkipping line 1348: expected 1 fields, saw 32\\nSkipping line 1353: expected 1 fields, saw 32\\nSkipping line 1358: expected 1 fields, saw 32\\nSkipping line 1363: expected 1 fields, saw 32\\nSkipping line 1368: expected 1 fields, saw 32\\nSkipping line 1373: expected 1 fields, saw 32\\nSkipping line 1378: expected 1 fields, saw 32\\nSkipping line 1383: expected 1 fields, saw 32\\nSkipping line 1388: expected 1 fields, saw 32\\nSkipping line 1393: expected 1 fields, saw 32\\nSkipping line 1398: expected 1 fields, saw 32\\nSkipping line 1403: expected 1 fields, saw 32\\nSkipping line 1408: expected 1 fields, saw 32\\nSkipping line 1413: expected 1 fields, saw 32\\nSkipping line 1418: expected 1 fields, saw 32\\nSkipping line 1423: expected 1 fields, saw 32\\nSkipping line 1428: expected 1 fields, saw 32\\nSkipping line 1433: expected 1 fields, saw 32\\nSkipping line 1438: expected 1 fields, saw 32\\nSkipping line 1443: expected 1 fields, saw 32\\nSkipping line 1448: expected 1 fields, saw 32\\nSkipping line 1453: expected 1 fields, saw 32\\nSkipping line 1458: expected 1 fields, saw 32\\nSkipping line 1463: expected 1 fields, saw 32\\nSkipping line 1468: expected 1 fields, saw 32\\nSkipping line 1473: expected 1 fields, saw 32\\nSkipping line 1478: expected 1 fields, saw 32\\nSkipping line 1483: expected 1 fields, saw 32\\nSkipping line 1488: expected 1 fields, saw 32\\nSkipping line 1493: expected 1 fields, saw 32\\nSkipping line 1498: expected 1 fields, saw 32\\nSkipping line 1503: expected 1 fields, saw 32\\nSkipping line 1508: expected 1 fields, saw 32\\nSkipping line 1513: expected 1 fields, saw 32\\nSkipping line 1518: expected 1 fields, saw 32\\nSkipping line 1523: expected 1 fields, saw 32\\nSkipping line 1528: expected 1 fields, saw 32\\nSkipping line 1533: expected 1 fields, saw 32\\nSkipping line 1538: expected 1 fields, saw 32\\nSkipping line 1543: expected 1 fields, saw 32\\nSkipping line 1548: expected 1 fields, saw 32\\nSkipping line 1553: expected 1 fields, saw 32\\nSkipping line 1558: expected 1 fields, saw 32\\nSkipping line 1563: expected 1 fields, saw 32\\nSkipping line 1568: expected 1 fields, saw 32\\nSkipping line 1573: expected 1 fields, saw 32\\nSkipping line 1578: expected 1 fields, saw 32\\nSkipping line 1583: expected 1 fields, saw 32\\nSkipping line 1588: expected 1 fields, saw 32\\nSkipping line 1593: expected 1 fields, saw 32\\nSkipping line 1598: expected 1 fields, saw 32\\nSkipping line 1603: expected 1 fields, saw 32\\nSkipping line 1608: expected 1 fields, saw 32\\nSkipping line 1613: expected 1 fields, saw 32\\nSkipping line 1618: expected 1 fields, saw 32\\nSkipping line 1623: expected 1 fields, saw 32\\nSkipping line 1628: expected 1 fields, saw 32\\nSkipping line 1633: expected 1 fields, saw 32\\nSkipping line 1638: expected 1 fields, saw 32\\nSkipping line 1643: expected 1 fields, saw 32\\nSkipping line 1648: expected 1 fields, saw 32\\nSkipping line 1653: expected 1 fields, saw 32\\nSkipping line 1658: expected 1 fields, saw 32\\nSkipping line 1663: expected 1 fields, saw 32\\nSkipping line 1668: expected 1 fields, saw 32\\nSkipping line 1673: expected 1 fields, saw 32\\nSkipping line 1678: expected 1 fields, saw 32\\nSkipping line 1683: expected 1 fields, saw 32\\nSkipping line 1688: expected 1 fields, saw 32\\nSkipping line 1693: expected 1 fields, saw 32\\nSkipping line 1698: expected 1 fields, saw 32\\nSkipping line 1703: expected 1 fields, saw 32\\nSkipping line 1708: expected 1 fields, saw 32\\nSkipping line 1713: expected 1 fields, saw 32\\nSkipping line 1718: expected 1 fields, saw 32\\nSkipping line 1723: expected 1 fields, saw 32\\nSkipping line 1728: expected 1 fields, saw 32\\nSkipping line 1733: expected 1 fields, saw 32\\nSkipping line 1738: expected 1 fields, saw 32\\nSkipping line 1743: expected 1 fields, saw 32\\nSkipping line 1748: expected 1 fields, saw 32\\nSkipping line 1753: expected 1 fields, saw 32\\nSkipping line 1758: expected 1 fields, saw 32\\nSkipping line 1763: expected 1 fields, saw 32\\nSkipping line 1768: expected 1 fields, saw 32\\nSkipping line 1773: expected 1 fields, saw 32\\nSkipping line 1778: expected 1 fields, saw 32\\nSkipping line 1783: expected 1 fields, saw 32\\nSkipping line 1788: expected 1 fields, saw 32\\nSkipping line 1793: expected 1 fields, saw 32\\nSkipping line 1798: expected 1 fields, saw 32\\nSkipping line 1803: expected 1 fields, saw 32\\nSkipping line 1808: expected 1 fields, saw 32\\nSkipping line 1813: expected 1 fields, saw 32\\nSkipping line 1818: expected 1 fields, saw 32\\nSkipping line 1823: expected 1 fields, saw 32\\nSkipping line 1828: expected 1 fields, saw 32\\nSkipping line 1833: expected 1 fields, saw 32\\nSkipping line 1838: expected 1 fields, saw 32\\nSkipping line 1843: expected 1 fields, saw 32\\nSkipping line 1848: expected 1 fields, saw 32\\nSkipping line 1853: expected 1 fields, saw 32\\nSkipping line 1858: expected 1 fields, saw 32\\nSkipping line 1863: expected 1 fields, saw 32\\nSkipping line 1868: expected 1 fields, saw 32\\nSkipping line 1873: expected 1 fields, saw 32\\nSkipping line 1878: expected 1 fields, saw 32\\nSkipping line 1883: expected 1 fields, saw 32\\nSkipping line 1888: expected 1 fields, saw 32\\nSkipping line 1893: expected 1 fields, saw 32\\nSkipping line 1898: expected 1 fields, saw 32\\nSkipping line 1903: expected 1 fields, saw 32\\nSkipping line 1908: expected 1 fields, saw 32\\nSkipping line 1913: expected 1 fields, saw 32\\nSkipping line 1918: expected 1 fields, saw 32\\nSkipping line 1923: expected 1 fields, saw 32\\nSkipping line 1928: expected 1 fields, saw 32\\nSkipping line 1933: expected 1 fields, saw 32\\nSkipping line 1938: expected 1 fields, saw 32\\nSkipping line 1943: expected 1 fields, saw 32\\nSkipping line 1948: expected 1 fields, saw 32\\nSkipping line 1953: expected 1 fields, saw 32\\nSkipping line 1958: expected 1 fields, saw 32\\nSkipping line 1963: expected 1 fields, saw 32\\nSkipping line 1968: expected 1 fields, saw 32\\nSkipping line 1973: expected 1 fields, saw 32\\nSkipping line 1978: expected 1 fields, saw 32\\nSkipping line 1983: expected 1 fields, saw 32\\nSkipping line 1988: expected 1 fields, saw 32\\nSkipping line 1993: expected 1 fields, saw 32\\nSkipping line 1998: expected 1 fields, saw 32\\nSkipping line 2003: expected 1 fields, saw 32\\nSkipping line 2008: expected 1 fields, saw 32\\nSkipping line 2013: expected 1 fields, saw 32\\nSkipping line 2018: expected 1 fields, saw 32\\nSkipping line 2023: expected 1 fields, saw 32\\nSkipping line 2028: expected 1 fields, saw 32\\nSkipping line 2033: expected 1 fields, saw 32\\nSkipping line 2038: expected 1 fields, saw 32\\nSkipping line 2043: expected 1 fields, saw 32\\nSkipping line 2048: expected 1 fields, saw 32\\nSkipping line 2053: expected 1 fields, saw 32\\nSkipping line 2058: expected 1 fields, saw 32\\nSkipping line 2063: expected 1 fields, saw 32\\nSkipping line 2068: expected 1 fields, saw 32\\nSkipping line 2073: expected 1 fields, saw 32\\nSkipping line 2078: expected 1 fields, saw 32\\nSkipping line 2083: expected 1 fields, saw 32\\nSkipping line 2088: expected 1 fields, saw 32\\nSkipping line 2093: expected 1 fields, saw 32\\nSkipping line 2098: expected 1 fields, saw 32\\nSkipping line 2103: expected 1 fields, saw 32\\nSkipping line 2108: expected 1 fields, saw 32\\nSkipping line 2113: expected 1 fields, saw 32\\nSkipping line 2118: expected 1 fields, saw 32\\nSkipping line 2123: expected 1 fields, saw 32\\nSkipping line 2128: expected 1 fields, saw 32\\nSkipping line 2133: expected 1 fields, saw 32\\nSkipping line 2138: expected 1 fields, saw 32\\nSkipping line 2143: expected 1 fields, saw 32\\nSkipping line 2148: expected 1 fields, saw 32\\nSkipping line 2153: expected 1 fields, saw 32\\nSkipping line 2158: expected 1 fields, saw 32\\nSkipping line 2163: expected 1 fields, saw 32\\nSkipping line 2168: expected 1 fields, saw 32\\nSkipping line 2173: expected 1 fields, saw 32\\nSkipping line 2178: expected 1 fields, saw 32\\nSkipping line 2183: expected 1 fields, saw 32\\nSkipping line 2188: expected 1 fields, saw 32\\nSkipping line 2193: expected 1 fields, saw 32\\nSkipping line 2198: expected 1 fields, saw 32\\nSkipping line 2203: expected 1 fields, saw 32\\nSkipping line 2208: expected 1 fields, saw 32\\nSkipping line 2213: expected 1 fields, saw 32\\nSkipping line 2218: expected 1 fields, saw 32\\nSkipping line 2223: expected 1 fields, saw 32\\nSkipping line 2228: expected 1 fields, saw 32\\nSkipping line 2233: expected 1 fields, saw 32\\nSkipping line 2238: expected 1 fields, saw 32\\nSkipping line 2243: expected 1 fields, saw 32\\nSkipping line 2248: expected 1 fields, saw 32\\nSkipping line 2253: expected 1 fields, saw 32\\nSkipping line 2258: expected 1 fields, saw 32\\nSkipping line 2263: expected 1 fields, saw 32\\nSkipping line 2268: expected 1 fields, saw 32\\nSkipping line 2273: expected 1 fields, saw 32\\nSkipping line 2278: expected 1 fields, saw 32\\nSkipping line 2283: expected 1 fields, saw 32\\nSkipping line 2288: expected 1 fields, saw 32\\nSkipping line 2293: expected 1 fields, saw 32\\nSkipping line 2298: expected 1 fields, saw 32\\nSkipping line 2303: expected 1 fields, saw 32\\nSkipping line 2308: expected 1 fields, saw 32\\nSkipping line 2313: expected 1 fields, saw 32\\nSkipping line 2318: expected 1 fields, saw 32\\nSkipping line 2323: expected 1 fields, saw 32\\nSkipping line 2328: expected 1 fields, saw 32\\nSkipping line 2333: expected 1 fields, saw 32\\nSkipping line 2338: expected 1 fields, saw 32\\nSkipping line 2343: expected 1 fields, saw 32\\nSkipping line 2348: expected 1 fields, saw 32\\nSkipping line 2353: expected 1 fields, saw 32\\nSkipping line 2358: expected 1 fields, saw 32\\nSkipping line 2363: expected 1 fields, saw 32\\nSkipping line 2368: expected 1 fields, saw 32\\nSkipping line 2373: expected 1 fields, saw 32\\nSkipping line 2378: expected 1 fields, saw 32\\nSkipping line 2383: expected 1 fields, saw 32\\nSkipping line 2388: expected 1 fields, saw 32\\nSkipping line 2393: expected 1 fields, saw 32\\nSkipping line 2398: expected 1 fields, saw 32\\nSkipping line 2403: expected 1 fields, saw 32\\nSkipping line 2408: expected 1 fields, saw 32\\nSkipping line 2413: expected 1 fields, saw 32\\nSkipping line 2418: expected 1 fields, saw 32\\nSkipping line 2423: expected 1 fields, saw 32\\nSkipping line 2428: expected 1 fields, saw 32\\nSkipping line 2433: expected 1 fields, saw 32\\nSkipping line 2438: expected 1 fields, saw 32\\nSkipping line 2443: expected 1 fields, saw 32\\nSkipping line 2448: expected 1 fields, saw 32\\nSkipping line 2453: expected 1 fields, saw 32\\nSkipping line 2458: expected 1 fields, saw 32\\nSkipping line 2463: expected 1 fields, saw 32\\nSkipping line 2468: expected 1 fields, saw 32\\nSkipping line 2473: expected 1 fields, saw 32\\nSkipping line 2478: expected 1 fields, saw 32\\nSkipping line 2483: expected 1 fields, saw 32\\nSkipping line 2488: expected 1 fields, saw 32\\nSkipping line 2493: expected 1 fields, saw 32\\nSkipping line 2498: expected 1 fields, saw 32\\nSkipping line 2503: expected 1 fields, saw 32\\nSkipping line 2508: expected 1 fields, saw 32\\nSkipping line 2513: expected 1 fields, saw 32\\nSkipping line 2518: expected 1 fields, saw 32\\nSkipping line 2523: expected 1 fields, saw 32\\nSkipping line 2528: expected 1 fields, saw 32\\nSkipping line 2533: expected 1 fields, saw 32\\nSkipping line 2538: expected 1 fields, saw 32\\nSkipping line 2543: expected 1 fields, saw 32\\nSkipping line 2548: expected 1 fields, saw 32\\nSkipping line 2553: expected 1 fields, saw 32\\nSkipping line 2558: expected 1 fields, saw 32\\nSkipping line 2563: expected 1 fields, saw 32\\nSkipping line 2568: expected 1 fields, saw 32\\nSkipping line 2573: expected 1 fields, saw 32\\nSkipping line 2578: expected 1 fields, saw 32\\nSkipping line 2583: expected 1 fields, saw 32\\nSkipping line 2588: expected 1 fields, saw 32\\nSkipping line 2593: expected 1 fields, saw 32\\nSkipping line 2598: expected 1 fields, saw 32\\nSkipping line 2603: expected 1 fields, saw 32\\nSkipping line 2608: expected 1 fields, saw 32\\nSkipping line 2613: expected 1 fields, saw 32\\nSkipping line 2618: expected 1 fields, saw 32\\nSkipping line 2623: expected 1 fields, saw 32\\nSkipping line 2628: expected 1 fields, saw 32\\nSkipping line 2633: expected 1 fields, saw 32\\nSkipping line 2638: expected 1 fields, saw 32\\nSkipping line 2643: expected 1 fields, saw 32\\nSkipping line 2648: expected 1 fields, saw 32\\nSkipping line 2653: expected 1 fields, saw 32\\nSkipping line 2658: expected 1 fields, saw 32\\nSkipping line 2663: expected 1 fields, saw 32\\nSkipping line 2668: expected 1 fields, saw 32\\nSkipping line 2673: expected 1 fields, saw 32\\nSkipping line 2678: expected 1 fields, saw 32\\nSkipping line 2683: expected 1 fields, saw 32\\nSkipping line 2688: expected 1 fields, saw 32\\nSkipping line 2693: expected 1 fields, saw 32\\nSkipping line 2698: expected 1 fields, saw 32\\nSkipping line 2703: expected 1 fields, saw 32\\nSkipping line 2708: expected 1 fields, saw 32\\nSkipping line 2713: expected 1 fields, saw 32\\nSkipping line 2718: expected 1 fields, saw 32\\nSkipping line 2723: expected 1 fields, saw 32\\nSkipping line 2728: expected 1 fields, saw 32\\nSkipping line 2733: expected 1 fields, saw 32\\nSkipping line 2738: expected 1 fields, saw 32\\nSkipping line 2743: expected 1 fields, saw 32\\nSkipping line 2748: expected 1 fields, saw 32\\nSkipping line 2753: expected 1 fields, saw 32\\nSkipping line 2758: expected 1 fields, saw 32\\nSkipping line 2763: expected 1 fields, saw 32\\nSkipping line 2768: expected 1 fields, saw 32\\nSkipping line 2773: expected 1 fields, saw 32\\nSkipping line 2778: expected 1 fields, saw 32\\nSkipping line 2783: expected 1 fields, saw 32\\nSkipping line 2788: expected 1 fields, saw 32\\nSkipping line 2793: expected 1 fields, saw 32\\nSkipping line 2798: expected 1 fields, saw 32\\nSkipping line 2803: expected 1 fields, saw 32\\nSkipping line 2808: expected 1 fields, saw 32\\nSkipping line 2813: expected 1 fields, saw 32\\nSkipping line 2818: expected 1 fields, saw 32\\nSkipping line 2823: expected 1 fields, saw 32\\nSkipping line 2828: expected 1 fields, saw 32\\nSkipping line 2833: expected 1 fields, saw 32\\nSkipping line 2838: expected 1 fields, saw 32\\nSkipping line 2843: expected 1 fields, saw 32\\nSkipping line 2848: expected 1 fields, saw 32\\nSkipping line 2853: expected 1 fields, saw 32\\nSkipping line 2858: expected 1 fields, saw 32\\nSkipping line 2863: expected 1 fields, saw 32\\nSkipping line 2868: expected 1 fields, saw 32\\nSkipping line 2873: expected 1 fields, saw 32\\nSkipping line 2878: expected 1 fields, saw 32\\nSkipping line 2883: expected 1 fields, saw 32\\nSkipping line 2888: expected 1 fields, saw 32\\nSkipping line 2893: expected 1 fields, saw 32\\nSkipping line 2898: expected 1 fields, saw 32\\nSkipping line 2903: expected 1 fields, saw 32\\nSkipping line 2908: expected 1 fields, saw 32\\nSkipping line 2913: expected 1 fields, saw 32\\nSkipping line 2918: expected 1 fields, saw 32\\nSkipping line 2923: expected 1 fields, saw 32\\nSkipping line 2928: expected 1 fields, saw 32\\nSkipping line 2933: expected 1 fields, saw 32\\nSkipping line 2938: expected 1 fields, saw 32\\nSkipping line 2943: expected 1 fields, saw 32\\nSkipping line 2948: expected 1 fields, saw 32\\nSkipping line 2953: expected 1 fields, saw 32\\nSkipping line 2958: expected 1 fields, saw 32\\nSkipping line 2963: expected 1 fields, saw 32\\nSkipping line 2968: expected 1 fields, saw 32\\nSkipping line 2973: expected 1 fields, saw 32\\nSkipping line 2978: expected 1 fields, saw 32\\nSkipping line 2983: expected 1 fields, saw 32\\nSkipping line 2988: expected 1 fields, saw 32\\nSkipping line 2993: expected 1 fields, saw 32\\nSkipping line 2998: expected 1 fields, saw 32\\nSkipping line 3003: expected 1 fields, saw 32\\nSkipping line 3008: expected 1 fields, saw 32\\nSkipping line 3013: expected 1 fields, saw 32\\nSkipping line 3018: expected 1 fields, saw 32\\nSkipping line 3023: expected 1 fields, saw 32\\nSkipping line 3028: expected 1 fields, saw 32\\nSkipping line 3033: expected 1 fields, saw 32\\nSkipping line 3038: expected 1 fields, saw 32\\nSkipping line 3043: expected 1 fields, saw 32\\nSkipping line 3048: expected 1 fields, saw 32\\nSkipping line 3053: expected 1 fields, saw 32\\nSkipping line 3058: expected 1 fields, saw 32\\nSkipping line 3063: expected 1 fields, saw 32\\nSkipping line 3068: expected 1 fields, saw 32\\nSkipping line 3073: expected 1 fields, saw 32\\nSkipping line 3078: expected 1 fields, saw 32\\nSkipping line 3083: expected 1 fields, saw 32\\nSkipping line 3088: expected 1 fields, saw 32\\nSkipping line 3093: expected 1 fields, saw 32\\nSkipping line 3098: expected 1 fields, saw 32\\nSkipping line 3103: expected 1 fields, saw 32\\nSkipping line 3108: expected 1 fields, saw 32\\nSkipping line 3113: expected 1 fields, saw 32\\nSkipping line 3118: expected 1 fields, saw 32\\nSkipping line 3123: expected 1 fields, saw 32\\nSkipping line 3128: expected 1 fields, saw 32\\nSkipping line 3133: expected 1 fields, saw 32\\nSkipping line 3138: expected 1 fields, saw 32\\nSkipping line 3143: expected 1 fields, saw 32\\nSkipping line 3148: expected 1 fields, saw 32\\nSkipping line 3153: expected 1 fields, saw 32\\nSkipping line 3158: expected 1 fields, saw 32\\nSkipping line 3163: expected 1 fields, saw 32\\nSkipping line 3168: expected 1 fields, saw 32\\nSkipping line 3173: expected 1 fields, saw 32\\nSkipping line 3178: expected 1 fields, saw 32\\nSkipping line 3183: expected 1 fields, saw 32\\nSkipping line 3188: expected 1 fields, saw 32\\nSkipping line 3193: expected 1 fields, saw 32\\nSkipping line 3198: expected 1 fields, saw 32\\nSkipping line 3203: expected 1 fields, saw 32\\nSkipping line 3208: expected 1 fields, saw 32\\nSkipping line 3213: expected 1 fields, saw 32\\nSkipping line 3218: expected 1 fields, saw 32\\nSkipping line 3223: expected 1 fields, saw 32\\nSkipping line 3228: expected 1 fields, saw 32\\nSkipping line 3233: expected 1 fields, saw 32\\nSkipping line 3238: expected 1 fields, saw 32\\nSkipping line 3243: expected 1 fields, saw 32\\nSkipping line 3248: expected 1 fields, saw 32\\nSkipping line 3253: expected 1 fields, saw 32\\nSkipping line 3258: expected 1 fields, saw 32\\nSkipping line 3263: expected 1 fields, saw 32\\nSkipping line 3268: expected 1 fields, saw 32\\nSkipping line 3273: expected 1 fields, saw 32\\nSkipping line 3278: expected 1 fields, saw 32\\nSkipping line 3283: expected 1 fields, saw 32\\nSkipping line 3288: expected 1 fields, saw 32\\nSkipping line 3293: expected 1 fields, saw 32\\nSkipping line 3298: expected 1 fields, saw 32\\nSkipping line 3303: expected 1 fields, saw 32\\nSkipping line 3308: expected 1 fields, saw 32\\nSkipping line 3313: expected 1 fields, saw 32\\nSkipping line 3318: expected 1 fields, saw 32\\nSkipping line 3323: expected 1 fields, saw 32\\nSkipping line 3328: expected 1 fields, saw 32\\nSkipping line 3333: expected 1 fields, saw 32\\nSkipping line 3338: expected 1 fields, saw 32\\nSkipping line 3343: expected 1 fields, saw 32\\nSkipping line 3348: expected 1 fields, saw 32\\nSkipping line 3353: expected 1 fields, saw 32\\nSkipping line 3358: expected 1 fields, saw 32\\nSkipping line 3363: expected 1 fields, saw 32\\nSkipping line 3368: expected 1 fields, saw 32\\nSkipping line 3373: expected 1 fields, saw 32\\nSkipping line 3378: expected 1 fields, saw 32\\nSkipping line 3383: expected 1 fields, saw 32\\nSkipping line 3388: expected 1 fields, saw 32\\nSkipping line 3393: expected 1 fields, saw 32\\nSkipping line 3398: expected 1 fields, saw 32\\nSkipping line 3403: expected 1 fields, saw 32\\nSkipping line 3408: expected 1 fields, saw 32\\nSkipping line 3413: expected 1 fields, saw 32\\nSkipping line 3418: expected 1 fields, saw 32\\nSkipping line 3423: expected 1 fields, saw 32\\nSkipping line 3428: expected 1 fields, saw 32\\nSkipping line 3433: expected 1 fields, saw 32\\nSkipping line 3438: expected 1 fields, saw 32\\nSkipping line 3443: expected 1 fields, saw 32\\nSkipping line 3448: expected 1 fields, saw 32\\nSkipping line 3453: expected 1 fields, saw 32\\nSkipping line 3458: expected 1 fields, saw 32\\nSkipping line 3463: expected 1 fields, saw 32\\nSkipping line 3468: expected 1 fields, saw 32\\nSkipping line 3473: expected 1 fields, saw 32\\nSkipping line 3478: expected 1 fields, saw 32\\nSkipping line 3483: expected 1 fields, saw 32\\nSkipping line 3488: expected 1 fields, saw 32\\nSkipping line 3493: expected 1 fields, saw 32\\nSkipping line 3498: expected 1 fields, saw 32\\nSkipping line 3503: expected 1 fields, saw 32\\nSkipping line 3508: expected 1 fields, saw 32\\nSkipping line 3513: expected 1 fields, saw 32\\nSkipping line 3518: expected 1 fields, saw 32\\nSkipping line 3523: expected 1 fields, saw 32\\nSkipping line 3528: expected 1 fields, saw 32\\nSkipping line 3533: expected 1 fields, saw 32\\nSkipping line 3538: expected 1 fields, saw 32\\nSkipping line 3543: expected 1 fields, saw 32\\nSkipping line 3548: expected 1 fields, saw 32\\nSkipping line 3553: expected 1 fields, saw 32\\nSkipping line 3558: expected 1 fields, saw 32\\nSkipping line 3563: expected 1 fields, saw 32\\nSkipping line 3568: expected 1 fields, saw 32\\nSkipping line 3573: expected 1 fields, saw 32\\nSkipping line 3578: expected 1 fields, saw 32\\nSkipping line 3583: expected 1 fields, saw 32\\nSkipping line 3588: expected 1 fields, saw 32\\nSkipping line 3637: expected 1 fields, saw 2\\nSkipping line 3638: expected 1 fields, saw 3\\nSkipping line 3639: expected 1 fields, saw 3\\nSkipping line 3640: expected 1 fields, saw 3\\nSkipping line 3641: expected 1 fields, saw 3\\nSkipping line 3642: expected 1 fields, saw 3\\nSkipping line 3649: expected 1 fields, saw 3\\nSkipping line 3650: expected 1 fields, saw 3\\nSkipping line 3651: expected 1 fields, saw 3\\nSkipping line 3652: expected 1 fields, saw 3\\nSkipping line 3653: expected 1 fields, saw 3\\nSkipping line 3654: expected 1 fields, saw 3\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjO8UB3Vct2y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d8217348-ad2c-4db8-8306-593a04c05aa2"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&lt;!DOCTYPE html&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;html lang=\"en\"&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;head&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;meta charset=\"utf-8\"&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;link rel=\"dns-prefetch\" href=\"https://githu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;link rel=\"dns-prefetch\" href=\"https://avata...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     <!DOCTYPE html>\n",
              "0                                   <html lang=\"en\">\n",
              "1                                             <head>\n",
              "2                             <meta charset=\"utf-8\">\n",
              "3    <link rel=\"dns-prefetch\" href=\"https://githu...\n",
              "4    <link rel=\"dns-prefetch\" href=\"https://avata..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54DVBcqTc4xR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "5bb738f7-febc-4bca-a28f-fabf665c3f71"
      },
      "source": [
        "dataset=dataset.drop([\"id\",\"Unnamed: 32\"],axis=1)\n",
        "dataset.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-24d24ed3cb52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Unnamed: 32\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['id' 'Unnamed: 32'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiIzvqevcT-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "1968fc8a-b349-4214-b232-8cdf9a3b268d"
      },
      "source": [
        "from keras.layers import Lambda, Activation\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_hmm import HMMTensorflow\n",
        "\n",
        "\n",
        "class HMMLayer(Layer):\n",
        "    def __init__(self, states, length=None, viterbi_inference=True, **kwargs):\n",
        "        # todo: perhaps states should just be inferred by the input shape\n",
        "        # todo: create a few utility functions for generating transition matrices\n",
        "        self.viterbi_inference = viterbi_inference\n",
        "        self.states = states\n",
        "        self.P = np.ones((states, states), dtype=np.float32) * (0.01 / (states - 1))\n",
        "        for i in range(states):\n",
        "            self.P[i, i] = 0.99\n",
        "\n",
        "        self.hmm = HMMTensorflow(self.P)\n",
        "\n",
        "        super(HMMLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if len(input_shape) != 3:\n",
        "            raise ValueError('input_shape must be 3, found {}'.format(len(input_shape)))\n",
        "\n",
        "        super(HMMLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
        "\n",
        "    def call(self, x):\n",
        "        # todo: only optionally apply sigmoid\n",
        "        # todo: apply viterbi during inference\n",
        "        x = Activation(K.sigmoid)(x)\n",
        "\n",
        "        # using K.in_train_phase results in both if and else conditions being\n",
        "        # computed, which in this case is very expensive. instead, tf.cond\n",
        "        # is used. Even so, if and else conditions must be wrapped in a lambda\n",
        "        # to ensure that they are not computed unless that path is chosen.\n",
        "        if self.viterbi_inference:\n",
        "            # include this in the graph so that keras knows that the learning phase\n",
        "            # variable needs to be passed into tensorflows session run.\n",
        "            x = K.in_train_phase(x, x)\n",
        "\n",
        "            return Lambda(lambda x: tf.cond(\n",
        "                K.learning_phase(),\n",
        "                lambda: self.hmm.forward_backward(x)[0],\n",
        "                lambda: self.hmm.viterbi_decode_batched(x, onehot=True)[0],\n",
        "            ))(x)\n",
        "        else:\n",
        "            return Lambda(lambda x: self.hmm.forward_backward(x)[0])(x)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6efe25b3952f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_hmm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHMMTensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hmm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpR8C2yYakCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7301933a-6051-4855-b4bf-0dec5f5817a4"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import pytest\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_hmm import HMMNumpy, HMMTensorflow\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def latch_P():\n",
        "    P = np.array([[0.5, 0.5], [0.0, 1.0]])\n",
        "    # P = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
        "    # P = np.array([[0.5, 0.5], [0.0000000001, 0.9999999999]])\n",
        "    # P = np.array([[0.5, 0.5], [1e-50, 1 - 1e-50]])\n",
        "\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            print('from', i, 'to', j, P[i, j])\n",
        "    return P\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def hmm_latch(latch_P):\n",
        "    return HMMNumpy(latch_P)\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def fair_P():\n",
        "    return np.array([[0.5, 0.5], [0.5, 0.5]])\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def hmm_fair(fair_P):\n",
        "    return HMMNumpy(fair_P)\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def hmm_tf_fair(fair_P):\n",
        "    return HMMTensorflow(fair_P)\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def hmm_tf_latch(latch_P):\n",
        "    return HMMTensorflow(latch_P)\n",
        "\n",
        "\n",
        "def lik(y):\n",
        "    \"\"\"\n",
        "    given 1d vector of likliehoods length N, return matrix with\n",
        "    shape (N, 2) where (N, 0) is 1 - y and (N, 1) is y.\n",
        "    given a 2d array of likelihood sequences of size [N, B] where B is the batch\n",
        "    size, return [B, N, 2] where out[B, N, 0] + out[B, N, 1] = 1\n",
        "    This makes it easy to convert a time series of probabilities\n",
        "    into 2 states, off/on, for a simple HMM.\n",
        "    \"\"\"\n",
        "\n",
        "    liklihood = np.array([y, y], float).T\n",
        "    liklihood[..., 0] = 1 - liklihood[..., 0]\n",
        "    return liklihood\n",
        "\n",
        "def test_tf_hmm_invalid_P_shape():\n",
        "    with pytest.raises(ValueError):\n",
        "        HMMTensorflow(np.ones((1, 2)))\n",
        "\n",
        "def test_tf_hmm_invalid_P_dimensions():\n",
        "    with pytest.raises(ValueError):\n",
        "        HMMTensorflow(np.ones((1,)))\n",
        "\n",
        "def test_hmm_tf_fair_forward_backward(hmm_tf_fair, hmm_fair):\n",
        "    y = lik(np.array([0, 0, 1, 1]))\n",
        "\n",
        "    np_posterior, _, _ = hmm_fair.forward_backward(y)\n",
        "    print('tf')\n",
        "    g_posterior, _, _ = hmm_tf_fair.forward_backward(y)\n",
        "    tf_posterior = np.concatenate(tf.Session().run(g_posterior))\n",
        "\n",
        "    print('np_posterior', np_posterior)\n",
        "    print('tf_posterior', tf_posterior)\n",
        "    assert np.isclose(np_posterior, tf_posterior).all()\n",
        "\n",
        "\n",
        "def test_hmm_tf_fair_forward_backward_multiple_batch(hmm_tf_fair, hmm_fair):\n",
        "    y = lik(np.array([0, 0, 1, 1]))\n",
        "    y = np.stack([y] * 3)\n",
        "\n",
        "    np_posterior, _, _ = hmm_fair.forward_backward(y)\n",
        "    print('tf')\n",
        "    g_posterior, _, _ = hmm_tf_fair.forward_backward(y)\n",
        "    tf_posterior = tf.Session().run(g_posterior)\n",
        "\n",
        "    print('np_posterior', np_posterior)\n",
        "    print('tf_posterior', tf_posterior)\n",
        "    assert np.isclose(np_posterior, tf_posterior).all()\n",
        "\n",
        "\n",
        "def test_hmm_tf_latch_forward_backward_multiple_batch(hmm_tf_latch, hmm_latch):\n",
        "    y = lik(np.array([0, 0, 1, 1]))\n",
        "    y = np.stack([y] * 3)\n",
        "\n",
        "    np_posterior, np_forward, np_backward = hmm_latch.forward_backward(y)\n",
        "    print('tf')\n",
        "    g_posterior, g_forward, g_backward = hmm_tf_latch.forward_backward(y)\n",
        "    tf_posterior = tf.Session().run(g_posterior)\n",
        "    tf_forward = tf.Session().run(g_forward)\n",
        "    tf_backward = tf.Session().run(g_backward)\n",
        "\n",
        "    assert np.isclose(np_forward, tf_forward).all()\n",
        "    print('np_backward', np_backward)\n",
        "    print('tf_backward', tf_backward)\n",
        "    assert np.isclose(np_backward, tf_backward).all()\n",
        "    print('np_posterior', np_posterior)\n",
        "    print('tf_posterior', tf_posterior)\n",
        "    assert np.isclose(np_posterior, tf_posterior).all()\n",
        "\n",
        "def test_lik():\n",
        "    yin = np.array([0, 0.25, 0.5, 0.75, 1])\n",
        "    y = lik(yin)\n",
        "\n",
        "    assert np.all(y == np.array([\n",
        "        [1.00, 0.00],\n",
        "        [0.75, 0.25],\n",
        "        [0.50, 0.50],\n",
        "        [0.25, 0.75],\n",
        "        [0.00, 1.00],\n",
        "    ]))\n",
        "\n",
        "\n",
        "def test_hmm_fair_forward_backward(hmm_fair):\n",
        "    y = lik(np.array([0, 0, 1, 1]))\n",
        "\n",
        "    posterior, f, b = hmm_fair.forward_backward(y)\n",
        "\n",
        "    # if P is filled with 0.5, the only thing that matters is the emission\n",
        "    # liklihood.  assert that the posterior is = the liklihood of y\n",
        "    for i, yi in enumerate(y):\n",
        "        liklihood = yi / np.sum(yi)\n",
        "        assert np.isclose(posterior[i, :], liklihood).all()\n",
        "\n",
        "    # assert that posterior for any given t sums to 1\n",
        "    assert np.isclose(np.sum(posterior, 1), 1).all()\n",
        "\n",
        "\n",
        "def test_hmm_latch_two_step_no_noise(hmm_latch):\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            y = [i, i, j, j]\n",
        "            # y = [i, j]\n",
        "\n",
        "            if i == 1 and j == 0:\n",
        "                continue\n",
        "\n",
        "            print('*'*80)\n",
        "            print(y)\n",
        "            states, scores = hmm_latch.viterbi_decode(lik(y))\n",
        "\n",
        "            assert all(states == y)\n",
        "\n",
        "\n",
        "def test_hmm_tf_partial_forward(hmm_tf_latch, hmm_latch):\n",
        "    scoress = [\n",
        "        np.log(np.array([0, 1])),\n",
        "        np.log(np.array([1, 0])),\n",
        "        np.log(np.array([0.25, 0.75])),\n",
        "        np.log(np.array([0.5, 0.5])),\n",
        "    ]\n",
        "\n",
        "    for scores in scoress:\n",
        "        tf_ret = tf.Session().run(\n",
        "            hmm_tf_latch._viterbi_partial_forward(scores)\n",
        "        )\n",
        "        np_ret = hmm_latch._viterbi_partial_forward(scores)\n",
        "\n",
        "        assert (tf_ret == np_ret).all()\n",
        "\n",
        "\n",
        "def test_hmm_tf_partial_forward_batched(hmm_tf_latch, hmm_latch):\n",
        "    scoress = [\n",
        "        np.log(np.array([0, 1])),\n",
        "        np.log(np.array([1, 0])),\n",
        "        np.log(np.array([0.25, 0.75])),\n",
        "        np.log(np.array([0.5, 0.5])),\n",
        "    ]\n",
        "\n",
        "    scores_batch = np.asarray(scoress)\n",
        "\n",
        "    np_res = hmm_latch._viterbi_partial_forward_batched(scores_batch)\n",
        "    tf_res = tf.Session().run(\n",
        "        hmm_tf_latch._viterbi_partial_forward_batched(scores_batch)\n",
        "    )\n",
        "\n",
        "    assert (tf_res == np_res).all()\n",
        "\n",
        "\n",
        "def test_hmm_partial_forward_batched(hmm_latch):\n",
        "    scoress = [\n",
        "        np.log(np.array([0, 1])),\n",
        "        np.log(np.array([1, 0])),\n",
        "        np.log(np.array([0.25, 0.75])),\n",
        "        np.log(np.array([0.5, 0.5])),\n",
        "    ]\n",
        "\n",
        "    scores_batch = np.array(scoress)\n",
        "\n",
        "    res = [hmm_latch._viterbi_partial_forward(scores) for scores in scoress]\n",
        "    res_batched = hmm_latch._viterbi_partial_forward_batched(scores_batch)\n",
        "\n",
        "    assert np.all(np.asarray(res) == res_batched)\n",
        "\n",
        "\n",
        "def test_hmm_tf_viterbi_decode(hmm_tf_latch, hmm_latch):\n",
        "    ys = [\n",
        "        lik(np.array([0, 0])),\n",
        "        lik(np.array([1, 1])),\n",
        "        lik(np.array([0, 1])),\n",
        "        lik(np.array([0, 0.25, 0.5, 0.75, 1])),\n",
        "    ]\n",
        "\n",
        "    for y in ys:\n",
        "        tf_s_graph, tf_scores_graph = hmm_tf_latch.viterbi_decode(y)\n",
        "        tf_s, tf_scores = tf.Session().run([tf_s_graph, tf_scores_graph])\n",
        "\n",
        "        np_s, np_scores = hmm_latch.viterbi_decode(y)\n",
        "\n",
        "        assert (tf_s == np_s).all()\n",
        "        assert (tf_scores == np_scores).all()\n",
        "\n",
        "\n",
        "def test_hmm_viterbi_decode_batched(hmm_latch):\n",
        "    ys_T2 = [\n",
        "        lik(np.array([0, 0])),\n",
        "        lik(np.array([0, 1])),\n",
        "        lik(np.array([1, 1])),\n",
        "    ]\n",
        "    ys_T5 = [\n",
        "        lik([0, 0.25, 0.5, 0.75, 1]),\n",
        "        lik([0, 0.65, 0.5, 0.95, .1]),\n",
        "    ]\n",
        "\n",
        "    ys_T2_batch = np.asarray(ys_T2)\n",
        "    ys_T5_batch = np.asarray(ys_T5)\n",
        "\n",
        "    res = [hmm_latch.viterbi_decode(y) for y in ys_T2]\n",
        "    res_s, res_scores = zip(*res)\n",
        "    res_s_batch, res_scores_batch = hmm_latch.viterbi_decode_batched(ys_T2_batch)\n",
        "    assert np.all(np.asarray(res_s) == res_s_batch)\n",
        "    assert np.all(np.asarray(res_scores) == res_scores_batch)\n",
        "\n",
        "    res = [hmm_latch.viterbi_decode(y) for y in ys_T5]\n",
        "    res_s, res_scores = zip(*res)\n",
        "    res_s_batch, res_scores_batch = hmm_latch.viterbi_decode_batched(ys_T5_batch)\n",
        "    assert np.all(np.asarray(res_s) == res_s_batch)\n",
        "    assert np.all(np.asarray(res_scores) == res_scores_batch)\n",
        "\n",
        "\n",
        "def test_hmm_tf_viterbi_decode_batched(hmm_tf_latch, hmm_latch):\n",
        "    ys_T2_batch = np.asarray([\n",
        "        lik(np.array([0, 0])),\n",
        "        lik(np.array([0, 1])),\n",
        "        lik(np.array([1, 1])),\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    ys_T5_batch = np.asarray([\n",
        "        lik([0, 0.25, 0.5, 0.75, 1]),\n",
        "        lik([0, 0.65, 0.5, 0.95, .1]),\n",
        "        lik([0, 0.25, 0.5, 0.75, 1]),\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    for y in (ys_T5_batch, ys_T2_batch):\n",
        "        np_res_s, np_res_scores = hmm_latch.viterbi_decode_batched(y)\n",
        "\n",
        "        y_variable = tf.placeholder(tf.float32, shape=(None, y.shape[1], y.shape[2]))\n",
        "        tf_s_graph, tf_scores_graph = hmm_tf_latch.viterbi_decode_batched(y_variable)\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        with tf.Session() as session:\n",
        "            session.run(init_op)\n",
        "\n",
        "            tf_s = session.run(tf_s_graph, {y_variable: y})\n",
        "            tf_scores = session.run(tf_scores_graph, {y_variable: y})\n",
        "\n",
        "        np.testing.assert_allclose(tf_s, np_res_s)\n",
        "        np.testing.assert_allclose(tf_scores, np_res_scores)\n",
        "\n",
        "\n",
        "def test_hmm_tf_viterbi_decode_wrong_shape(hmm_tf_latch, hmm_latch):\n",
        "    with pytest.raises(ValueError):\n",
        "        hmm_tf_latch.viterbi_decode([0, 1, 1, 0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e93e2eb3392a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_hmm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHMMNumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHMMTensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hmm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MsH_lL39IpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "8cef4a5c-4a10-427a-f0b7-8929c8564f4f"
      },
      "source": [
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model \n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras import backend as k \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
        "\n",
        "img_width, img_height = 256, 256\n",
        "train_data_dir = \"data/train\"\n",
        "validation_data_dir = \"data/val\"\n",
        "nb_train_samples = 4125\n",
        "nb_validation_samples = 466 \n",
        "batch_size = 16\n",
        "epochs = 50\n",
        "\n",
        "model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))\n",
        "\n",
        "\"\"\"\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
        "_________________________________________________________________\n",
        "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
        "_________________________________________________________________\n",
        "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
        "_________________________________________________________________\n",
        "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
        "_________________________________________________________________\n",
        "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
        "_________________________________________________________________\n",
        "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
        "_________________________________________________________________\n",
        "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
        "_________________________________________________________________\n",
        "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
        "_________________________________________________________________\n",
        "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
        "_________________________________________________________________\n",
        "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
        "_________________________________________________________________\n",
        "block3_conv4 (Conv2D)        (None, 64, 64, 256)       590080    \n",
        "_________________________________________________________________\n",
        "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
        "_________________________________________________________________\n",
        "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
        "_________________________________________________________________\n",
        "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "block4_conv4 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
        "_________________________________________________________________\n",
        "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "block5_conv4 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
        "=================================================================\n",
        "Total params: 20,024,384.0\n",
        "Trainable params: 20,024,384.0\n",
        "Non-trainable params: 0.0\n",
        "\"\"\"\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model.layers[:5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "#Adding custom Layers \n",
        "x = model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "predictions = Dense(16, activation=\"softmax\")(x)\n",
        "\n",
        "# creating the final model \n",
        "model_final = Model(input = model.input, output = predictions)\n",
        "\n",
        "# compile the model \n",
        "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "\n",
        "# Initiate the train and test generators with data Augumentation \n",
        "train_datagen = ImageDataGenerator(\n",
        "rescale = 1./255,\n",
        "horizontal_flip = True,\n",
        "fill_mode = \"nearest\",\n",
        "zoom_range = 0.3,\n",
        "width_shift_range = 0.3,\n",
        "height_shift_range=0.3,\n",
        "rotation_range=30)\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "rescale = 1./255,\n",
        "horizontal_flip = True,\n",
        "fill_mode = \"nearest\",\n",
        "zoom_range = 0.3,\n",
        "width_shift_range = 0.3,\n",
        "height_shift_range=0.3,\n",
        "rotation_range=30)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "train_data_dir,\n",
        "target_size = (img_height, img_width),\n",
        "batch_size = batch_size, \n",
        "class_mode = \"categorical\")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "validation_data_dir,\n",
        "target_size = (img_height, img_width),\n",
        "class_mode = \"categorical\")\n",
        "\n",
        "# Save the model according to the conditions  \n",
        "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "\n",
        "\n",
        "# Train the model \n",
        "model_final.fit_generator(\n",
        "train_generator,\n",
        "samples_per_epoch = nb_train_samples,\n",
        "epochs = epochs,\n",
        "validation_data = validation_generator,\n",
        "nb_val_samples = nb_validation_samples,\n",
        "callbacks = [checkpoint, early])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 4s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9e28acee40ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m class_mode = \"categorical\")\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m validation_generator = test_datagen.flow_from_directory(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         )\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaYmAG3SLhdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "c32219f6-d286-4288-98bc-6580f4679612"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "def clean_review(text):\n",
        "    # Strip HTML tags\n",
        "    text = re.sub('<[^<]+?>', ' ', text)\n",
        " \n",
        "    # Strip escaped quotes\n",
        "    text = text.replace('\\\\\"', '')\n",
        " \n",
        "    # Strip quotes\n",
        "    text = text.replace('\"', '')\n",
        " \n",
        "    return text\n",
        " \n",
        "df = pd.read_csv('labeledTrainData.csv', sep='\\t', quoting=3)\n",
        "df['cleaned_review'] = df['review'].apply(clean_review)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_review'], df['sentiment'], test_size=0.2)\n",
        " "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5f173c860899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labeledTrainData.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_review'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'labeledTrainData.csv' does not exist: b'labeledTrainData.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwkEQD2XMCr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vBUXuOqL-pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I_uw1ccL7Gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8NVMOQALi8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "outputId": "81743ed7-9374-41f5-b7d9-c5231dd01bc5"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n",
        "                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n",
        "X_train_onehot = vectorizer.fit_transform(X_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-aef751648bef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n\u001b[0m\u001b[1;32m      5\u001b[0m                              lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf4Out0aLnbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guf4j40n9J8o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "4f02e08d-787d-4207-ed13-66853e346b04"
      },
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model = keras.models.Sequential()\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Dense(units=500, activation='relu', input_dim=len(vectorizer.get_feature_names())))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        " \n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-46b5891370f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7jESc5cIzSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWbU_OtaIpjX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "916d9591-f5b1-449e-a263-b381ff5731f2"
      },
      "source": [
        "import keras.model.Sequential"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4accf0f0c3d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBjvK0_qIxx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "05253ac9-8212-43a2-8790-2546cc7d91e9"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, activation=\"relu\", input_shape=(32, 32, 3)))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 32, 32, 512)       2048      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32, 32, 10)        5130      \n",
            "=================================================================\n",
            "Total params: 7,178\n",
            "Trainable params: 7,178\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X_5V70mRSEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "1902febb-77cc-4ceb-cf9b-0c2aaebb7f3e"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,250,858\n",
            "Trainable params: 1,250,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwGH32XYRTZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "d1a57410-191d-43fc-be35-e0df97d71f8b"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 30, 30, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 512)               25690624  \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 25,761,322\n",
            "Trainable params: 25,761,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgZ9WgHvRZig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "8ae84210-1f75-4633-ebd1-068a505c94cd"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,250,858\n",
            "Trainable params: 1,250,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlD9YGVCRfGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NBQMvpONejc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3), activation=\"relu\"))\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n",
        "from keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of names for each CIFAR10 class\n",
        "cifar10_class_names = {\n",
        "    0: \"Plane\",\n",
        "    1: \"Car\",\n",
        "    2: \"Bird\",\n",
        "    3: \"Cat\",\n",
        "    4: \"Deer\",\n",
        "    5: \"Dog\",\n",
        "    6: \"Frog\",\n",
        "    7: \"Horse\",\n",
        "    8: \"Boat\",\n",
        "    9: \"Truck\"\n",
        "}\n",
        "\n",
        "# Load the entire data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Loop through each picture in the data set\n",
        "for i in range(1000):\n",
        "    # Grab an image from the data set\n",
        "    sample_image = x_train[i]\n",
        "    # Grab the image's expected class id\n",
        "    image_class_number = y_train[i][0]\n",
        "    # Look up the class name from the class id\n",
        "    image_class_name = cifar10_class_names[image_class_number]\n",
        "\n",
        "    # Draw the image as a plot\n",
        "    plt.imshow(sample_image)\n",
        "    # Label the image\n",
        "    plt.title(image_class_name)\n",
        "    # Show the plot on the screen\n",
        "    plt.show()\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "# Our labels are single values from 0 to 9.\n",
        "# Instead, we want each label to be an array with on element set to 1 and and the rest set to 0.\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train =\n",
        "x_test =\n",
        "x_train =\n",
        "x_test =\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "# Our labels are single values from 0 to 9.\n",
        "# Instead, we want each label to be an array with on element set to 1 and and the rest set to 0.\n",
        "y_train =\n",
        "y_test =\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation=\"relu\", input_shape=(32, 32, 3)))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create a model and add layers\n",
        "model =\n",
        "\n",
        "\n",
        "# Print a summary of the model\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data set to 0-to-1 range\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Create a model and add layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# Print a summary of the model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}